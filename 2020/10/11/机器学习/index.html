<!DOCTYPE html>
<html>
  <head><meta name="generator" content="Hexo 3.9.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="My personal blog">
  <meta name="keyword" content="hexo-theme, vuejs">
  
    <link rel="shortcut icon" href="/css/images/logo.png">
  
  <title>
    
      机器学习 | Mars&#39;s Blog
    
  </title>
  <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script>
  
  
  
  
    <!-- MathJax support START -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- MathJax support END -->
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.png">
</div>

  <body>
    <header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Mars's Blog</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/tags/" class="item-link">Tags</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/project/" class="item-link">Projects</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/tags/" class="menu-link">Tags</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/project/" class="menu-link">Projects</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>机器学习</h2>
  <p class="post-date">2020-10-11</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><h1 id="1-机器学习概论"><a href="#1-机器学习概论" class="headerlink" title="1. 机器学习概论"></a>1. 机器学习概论</h1><blockquote>
<p><a href="https://www.cnblogs.com/ylHe/p/9336719.html" target="_blank" rel="noopener">https://www.cnblogs.com/ylHe/p/9336719.html</a></p>
</blockquote>
<p>机器学习是人工智能三大基石之一</p>
<p>1推理 2知识 3学习</p>
<p>特点：利用经验改善系统的性能</p>
<h5 id="经典机器学习定义："><a href="#经典机器学习定义：" class="headerlink" title="经典机器学习定义："></a>经典机器学习定义：</h5><p>蕴含特定目的的知识的获取过程，内部表现为新知识的不断建立和修正，外部表现为性能改善。</p>
<p><strong>经验</strong>表现为数据和常识的形式</p>
<h5 id="现代统计机器学习定义："><a href="#现代统计机器学习定义：" class="headerlink" title="现代统计机器学习定义："></a>现代统计机器学习定义：</h5><p>任何<strong>通过数据训练</strong>学习的算法都属于机器学习技术</p>
<h5 id="学习系统："><a href="#学习系统：" class="headerlink" title="学习系统："></a>学习系统：</h5><img src="../../../img/image-20200722101124392.png" alt="image-20200722101124392" style="zoom: 33%;">

<p>学习算法在统计学中可能是一些优化算法，比如随机梯度下降等，在机器学习的范畴中可能范围更大</p>
<p>机器学习算法从数据中学到模型，在模型空间中学哪个模型最合适，这个就是学得模型</p>
<p>三要素：</p>
<ul>
<li>输入数据</li>
<li>模型空间</li>
<li>机器学习算法本身</li>
</ul>
<p>不同的数据产生不同的学习算法（数据驱动角度）</p>
<p>机器学习算法从数据出发，选择合适的算法，数据是第一位</p>
<h5 id="模型和学习层面"><a href="#模型和学习层面" class="headerlink" title="模型和学习层面"></a>模型和学习层面</h5><h6 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h6><ul>
<li><p>形式：线性/非线性</p>
</li>
<li><p>体系：浅层/深层/递归</p>
</li>
</ul>
<img src="../../../img/image-20200725085547934.png" alt="image-20200725085547934" style="zoom:33%;">

<blockquote>
<p><a href="https://www.cnblogs.com/toone/p/8574294.html" target="_blank" rel="noopener">https://www.cnblogs.com/toone/p/8574294.html</a></p>
</blockquote>
<p>模型空间可能是SVM，决策树等</p>
<p>体系：神经网络层数   做NLP，语音递归</p>
<h6 id="学习层面："><a href="#学习层面：" class="headerlink" title="学习层面："></a>学习层面：</h6><ul>
<li><p>经典（90c前）</p>
</li>
<li><p>现代  </p>
<p>监督学习，统计学习，集成学习…</p>
</li>
<li><p>混合</p>
</li>
</ul>
<img src="../../../img/image-20200725092224814.png" alt="image-20200725092224814" style="zoom:25%;">

<p>机器学习是人工智能的一个分支，深度学习是机器学习的分支</p>
<h5 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h5><p>监督学习 找x到y的f(x)</p>
<ul>
<li>回归 </li>
<li>分类</li>
</ul>
<h3 id="系统建模和模型选择"><a href="#系统建模和模型选择" class="headerlink" title="系统建模和模型选择"></a>系统建模和模型选择</h3><h6 id="维数灾难"><a href="#维数灾难" class="headerlink" title="维数灾难"></a>维数灾难</h6><p>随着输入维数的增加，算法需要更多的数据</p>
<p>所需的数据/样本空间的数据 越来越大</p>
<blockquote>
<p>设随机变量服从均匀分布，如果一维空间（即数轴上的某一区间）需要N个样本才能完全覆盖，那么当二维空间下还是N个样本时，覆盖度就有所下降。随着维度的增加，覆盖度指数级下降。</p>
<p>在样本量一定的情况下，维度越高，样本在空间中的分布越呈现稀疏性。</p>
</blockquote>
<h5 id="数据集划分方法"><a href="#数据集划分方法" class="headerlink" title="数据集划分方法"></a>数据集划分方法</h5><p>留出法、交叉验证</p>
<p>验证集：模型的选择</p>
<p>测试集：模型的评估</p>
<h4 id="建模相关："><a href="#建模相关：" class="headerlink" title="建模相关："></a>建模相关：</h4><ul>
<li><p>函数/模型的刻画</p>
<ul>
<li>f(x) 怎么来？给一堆数据后，不知道如何处理因此做先验（比如对于CNN，输入输出有多少结点）</li>
</ul>
</li>
<li><p>找模型参数 - 定义目标/损失函数 【学习算法本身】</p>
</li>
<li><p>评估泛化性能 在未知样本上的性能</p>
</li>
</ul>
<h4 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h4><h5 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h5><p>对训练集拟合的越来越好，测试集精度会越来越大（训练样本和测试样本精度不是吻合的）</p>
<p>欠拟合(under-fitting) 指产生一些误分</p>
<p>好拟合(good-fitting)</p>
<h5 id="共性问题"><a href="#共性问题" class="headerlink" title="共性问题"></a>共性问题</h5><p>有限样本，模型数远远大于样本数</p>
<blockquote>
<p>线性代数中的病态问题：1，3，5 ? ?</p>
</blockquote>
<p>导致的问题是即使训练误差小，但泛化性能未必好</p>
<h4 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h4><h6 id="评价指标很重要，但指标又和数据有关系，相互影响"><a href="#评价指标很重要，但指标又和数据有关系，相互影响" class="headerlink" title="评价指标很重要，但指标又和数据有关系，相互影响"></a>评价指标很重要，但指标又和数据有关系，相互影响</h6><img src="../../../img/image-20200726120242789.png" alt="image-20200726120242789" style="zoom: 25%;">

<h5 id="精度矩阵"><a href="#精度矩阵" class="headerlink" title="精度矩阵"></a>精度矩阵</h5><p>TP （T表示预测是否正确，P表示预测的类别）读作真正例、假负例、假正例、真负例</p>
<p>对角线表示<strong>预测</strong>正确，不是结果正确</p>
<p>区别在于行列的表示刚好相反</p>
<img src="../../../img/image-20200726121002301.png" alt="image-20200726121002301" style="zoom:50%;">



<h5 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h5><img src="../../../img/image-20200726134416811.png" alt="image-20200726134416811" style="zoom: 33%;">

<h5 id="不平衡数据集"><a href="#不平衡数据集" class="headerlink" title="不平衡数据集"></a>不平衡数据集</h5><p>敏感性高=漏诊率低<br>特异性低=误诊率高</p>
<h6 id="MCC-马修斯相关系数（衡量二分类）"><a href="#MCC-马修斯相关系数（衡量二分类）" class="headerlink" title="MCC 马修斯相关系数（衡量二分类）"></a>MCC 马修斯相关系数（衡量二分类）</h6><p>F1不能衡量不平衡数据集，使用MCC来度量</p>
<p><img src="../../../img/image-20200726194340201.png" alt="image-20200726194340201"></p>
<blockquote>
<p><strong>MCC</strong>（Matthews correlation coefficient）：是应用在机器学习中，用以测量二分类的分类性能的指标[83]。该指标考虑了真阳性、真阴性和假阳性和假阴性，通常认为该指标是一个比较均衡的指标，即使是在两类别的样本含量差别很大时，也可以应用它。MCC本质上是一个描述实际分类与预测分类之间的相关系数，它的取值范围为[-1,1]，取值为1时表示对受试对象的完美预测，取值为0时表示预测的结果还不如随机预测的结果，-1是指预测分类和实际分类完全不一致。</p>
</blockquote>
<h6 id="对数损失（可以来衡量多分类）"><a href="#对数损失（可以来衡量多分类）" class="headerlink" title="对数损失（可以来衡量多分类）"></a>对数损失（可以来衡量多分类）</h6><img src="../../../img/image-20200726201305581.png" alt="image-20200726201305581" style="zoom: 33%;">





<p>TP曲线越高，面积越大，分类器性能越好。</p>
<h5 id="测量精度"><a href="#测量精度" class="headerlink" title="测量精度"></a>测量精度</h5><p>评价一个系统的工业表现：是否稳定？也就是是否有可重复性？</p>
<p>可重复性：（类似输入，是否产生相近输出）</p>
<p>测量精度类似于概率分布的方差</p>
<p>精度是指系统本身对一个问题的测量程度（和前面的精度不一样）</p>
<p>2 方差高</p>
<p>4 既不准，又落得很开</p>
<h4 id="模型选择-选择一个具有良好泛化性能的模型"><a href="#模型选择-选择一个具有良好泛化性能的模型" class="headerlink" title="模型选择 - 选择一个具有良好泛化性能的模型"></a>模型选择 - 选择一个具有良好泛化性能的模型</h4><p><strong>没有免费午餐定理</strong></p>
<p>没有天生优越的学习器，充分利用了问题先验知识的模型才可能最优</p>
<h5 id="四类方法："><a href="#四类方法：" class="headerlink" title="四类方法："></a>四类方法：</h5><ul>
<li><p>模型选择 交叉验证</p>
</li>
<li><p>正则化 + 先验</p>
<p><strong>病态→良态</strong>，将解空间约束在某个范围</p>
<p>正则化：规范化，防止过拟合，增加泛化能力。也就是在目标函数上加一些限制</p>
<p>良态问题满足的条件：</p>
<ul>
<li>存在性</li>
<li>唯一性</li>
<li>连续/稳定</li>
</ul>
</li>
<li><p>模型的组合/集成</p>
</li>
<li><p>多数据视图</p>
</li>
</ul>
<p>机器学习都是病态问题，给定的数据有限，要求在高规模下解决</p>
<h5 id="吉洪诺夫正则化（Tikhonov）"><a href="#吉洪诺夫正则化（Tikhonov）" class="headerlink" title="吉洪诺夫正则化（Tikhonov）"></a>吉洪诺夫正则化（Tikhonov）</h5><p>非负范函/先验/正则化来辅助</p>
<p>例：CNN权衰减/dropout</p>
<h5 id="先验的重要性"><a href="#先验的重要性" class="headerlink" title="先验的重要性"></a>先验的重要性</h5><p>机器学习追求泛化的最好性能</p>
<h6 id="泛化-数据-知识（与领域-任务无关的叫源知识）"><a href="#泛化-数据-知识（与领域-任务无关的叫源知识）" class="headerlink" title="泛化 = 数据 + 知识（与领域/任务无关的叫源知识）"></a>泛化 = 数据 + 知识（与领域/任务无关的叫源知识）</h6><p>例：输入输出的映射应该光滑（相似输入 对应 相似输出）</p>
<h5 id="丑小鸭定理-特征表示"><a href="#丑小鸭定理-特征表示" class="headerlink" title="丑小鸭定理 - 特征表示"></a>丑小鸭定理 - 特征表示</h5><p>与任务相关的特征才是好特征，抛开任务谈数据是没有意义的。</p>
<blockquote>
<p>只有确定了任务，获得什么任务，提取什么特征才有意义。</p>
</blockquote>
<h3 id="统计学相关概念"><a href="#统计学相关概念" class="headerlink" title="统计学相关概念"></a>统计学相关概念</h3><p><em>数学概念记录下来</em></p>
<p>方差 针对单个统计量</p>
<p>协方差 针对两个统计量<br>$$<br>cov({𝐱_𝑖 },{𝐲_𝑖 })=𝐸({𝐱_𝑖 }−𝝁)𝐸({𝐲_𝑖 }−𝒗)<br>$$<br>协方差矩阵 针对多个统计量 对角线上是单个变量方差，非对角线是行列的两个变量的协方差。</p>
<blockquote>
<p> 理解协方差矩阵中对角线和非对角线上值的物理意义，=0、&gt;0、&lt;0的含义</p>
<p> =0 不相关 &gt;0 正相关 &lt;0 负相关</p>
</blockquote>
<h5 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h5><img src="../../../img/image-20200727102733377.png" alt="image-20200727102733377" style="zoom:25%;">

<img src="../../../img/image-20200727103342851.png" alt="image-20200727103342851" style="zoom:25%;">

<p>修正了欧式距离中各个维度尺度不一致且相关的问题</p>
<p>其中Σ是多维随机变量的协方差矩阵，μ为样本均值，<u>如果协方差矩阵是单位向量，也就是各维度独立同分布，马氏距离就变成了欧氏距离</u>。</p>
<p>协方差矩阵的用途 同时考虑均值和方差</p>
<p>例：某个点和其他的距离是多少？</p>
<p>对其他点求均值点，然后计算均值点和该点的距离。</p>
<h5 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h5><p>单位阵：一维 非对角线都为0 中心聚集，离中心点越远，数据点越少 （单位矩阵，表示x和其他点没有关系）</p>
<p>对角阵：二维 同样只有对角线上有值  x1，x2方向的方差不一样就会形成椭圆形</p>
<p>一般阵：离散化  - 协方差所有点都有值 x1，x2等存在一定的相关性</p>
<p>值集中在对角线</p>
<ul>
<li>对角线全相等（1） x y轴方差相同   </li>
<li>对角线不等（2）</li>
</ul>
<blockquote>
<p>这里的理解可以参考马同学的解释</p>
<img src="../../../img/image-20200727174703181.png" alt="image-20200727174703181" style="zoom:33%;">
</blockquote>
<h5 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h5><p><strong>交叉区域</strong>的样本可以通过先验概率来判定。</p>
<h5 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h5><p>当知道某个特征x的取值，再判断是哪个类别 — 后验概率</p>
<p>贝叶斯法则  结果为后验概率 </p>
<img src="https://tva1.sinaimg.cn/large/0082zybply1gc2xk3peb7j30q8076abp.jpg" alt="image-20200220160838284" style="zoom:25%;">

<p>最大化后验：甚至不需要计算分母P(Xj)，因为可以直接比较各个类别可能的概率</p>
<h5 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h5><p>数据量比较少的时候，失去了统计上的显著性。</p>
<p>这种情况下，需要较大的样本【高维下面临的维数灾难】</p>
<p>做假设：每个属性假设相互独立，计算连乘</p>
<h5 id="Bias-Variance-两难"><a href="#Bias-Variance-两难" class="headerlink" title="Bias - Variance 两难"></a>Bias - Variance 两难</h5><p>y 和 f(x)有偏差（噪声的影响）</p>
<p>f帽 在不同数据点产生不同的值（测量精度的原因）</p>
<img src="../笔记合集/img/image-20200220162040419.png" alt="image-20200220162040419" style="zoom:25%;">



<p>尽可能让前面的bias和Variance尽可能小。</p>
<h5 id="模型选择的次序："><a href="#模型选择的次序：" class="headerlink" title="模型选择的次序："></a><strong>模型选择的次序：</strong></h5><p>方差小，偏差小  →  方差小，偏差大  →  方差大，偏差小  →  方差大，偏差大</p>
<p>2优于3的原因：方差小说明泛化性能比较稳定，偏差大是因为样本量不够。方差大说明模型就不行。</p>
<h3 id="新型机器学习技术"><a href="#新型机器学习技术" class="headerlink" title="新型机器学习技术"></a>新型机器学习技术</h3><h5 id="本质（统计机器学习）"><a href="#本质（统计机器学习）" class="headerlink" title="本质（统计机器学习）"></a>本质（统计机器学习）</h5><p>有限样本对未知数据进行建模，涉及特征，模型，优化</p>
<ul>
<li>终身/连续学习  训练完模型后不再学习，怎样在生产环境中不断地学</li>
<li>迁移学习            训练集和测试集的分布不同</li>
<li>强化学习            在某个场景下找出最优的策略 机理不同于传统分类、回归等</li>
<li>对抗学习            没有足够多样本，想要生成足够逼真的样本</li>
<li>元学习                 关于学习的学习</li>
</ul>
<h4 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h4><p><u>工业场景</u>要求越来越小，大，深，快</p>
<ul>
<li>模型层面</li>
<li>优化层面<ul>
<li>允许不断调整 - online</li>
<li>分布式，并行等快速收敛</li>
<li>凸优化</li>
</ul>
</li>
<li>数据层面</li>
</ul>
<h1 id="2-神经元和感知机"><a href="#2-神经元和感知机" class="headerlink" title="2. 神经元和感知机"></a>2. 神经元和感知机</h1><h3 id="2-1-脑和神经元"><a href="#2-1-脑和神经元" class="headerlink" title="2.1 脑和神经元"></a>2.1 脑和神经元</h3><img src="../../../img/image-20200728194217226.png" alt="image-20200728194217226" style="zoom: 33%;">

<p>连接学派 神经网络</p>
<p>通过节点之间的连接实现任务</p>
<h4 id="Hebb法则"><a href="#Hebb法则" class="headerlink" title="Hebb法则"></a>Hebb法则</h4><p>连接强度的调整量与<strong>输入输出</strong>的乘积成正比（需要输入、输出都强）</p>
<p>连接可以通过外界的方式加强</p>
<p>巴普洛夫：听觉细胞 - 吃的细胞 - 神经元形成了强壮的连接</p>
<p>长程增强机制 LTP（后天外界可以增强神经元之间的连接）</p>
<h4 id="神经网络发展历史"><a href="#神经网络发展历史" class="headerlink" title="神经网络发展历史"></a>神经网络发展历史</h4><p>1943 MP模型 解决可计算性问题</p>
<p>1969 非线性自适应网络</p>
<p>1982-1984 发表ANN文章</p>
<p>2006 推进深度学习</p>
<h4 id="MP神经元"><a href="#MP神经元" class="headerlink" title="MP神经元"></a>MP神经元</h4><p>构建神经元的初步目的：解决逻辑上的OR，XOR等问题</p>
<img src="../../../img/image-20200728202721560.png" alt="image-20200728202721560" style="zoom:25%;">

<h5 id="基本模型"><a href="#基本模型" class="headerlink" title="基本模型"></a>基本模型</h5><p>偏置单元是x0，对应权值是w0</p>
<p>加权求和相当于突触</p>
<p>突破阈值后，神经元激活往外放电</p>
<p>神经元的输出就是w和x的向量相乘的结果，如果低于阈值，那么神经元处于抑制状态；为正则为兴奋状态。</p>
<p>与：x+y-2 或：x+y-1  阈值都为0</p>
<h5 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h5><p>现实生活中的神经元：</p>
<ul>
<li>输入可能是非线性</li>
<li>输出一般是脉冲序列</li>
<li>随机异步更新（我们模型是一个时钟脉冲来更新）</li>
<li>只存在权值为正的兴奋连接和权值为负的抑制连接，不存在正负转换</li>
</ul>
<h5 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h5><ul>
<li><p>饱和型 tanh, Sigmoid （值域受限）</p>
<p>缺点：</p>
<ul>
<li>梯度消失</li>
<li>非以0为中心</li>
<li>指数计算代价大</li>
</ul>
</li>
<li><p>非饱和型 ReLU  深度学习一般使用非饱和型，主要是因为指数计算代价和梯度对于深度学习影响比较大</p>
</li>
</ul>
<img src="../笔记合集/img/image-20200220172617196.png" alt="image-20200220172617196" style="zoom:25%;">

<p>神经元 → 网络</p>
<p>因为单神经元不能学习，输入输出不变</p>
<h3 id="2-2-感知器和感知学习"><a href="#2-2-感知器和感知学习" class="headerlink" title="2.2 感知器和感知学习"></a>2.2 感知器和感知学习</h3><p><strong>感知器 - 【二元】【线性】【分类】器</strong></p>
<p>感知器可以在二维或者多维，学到的是二元的决策边界</p>
<p>二元指的是结果为y/n</p>
<p>与神经元的区别：</p>
<ul>
<li>神经元只有x，y的输入，而Perception有多组向量输入</li>
<li>与逻辑，或逻辑中的权重已经设置好，而感知器不设置，“学习”从这里开始</li>
</ul>
<h4 id="感知器结构"><a href="#感知器结构" class="headerlink" title="感知器结构"></a>感知器结构</h4><h6 id="MP神经元组成的集合-输出只有一个"><a href="#MP神经元组成的集合-输出只有一个" class="headerlink" title="MP神经元组成的集合  输出只有一个"></a>MP神经元组成的集合  输出只有一个</h6><img src="../../../img/image-20200729084604060.png" alt="image-20200729084604060" style="zoom:50%;">

<p>特点：每个神经元都是独立的，权值也可以是独立的，唯一共享的就是输入。</p>
<p>$$w_{ij}$$表示输入结点i到神经元j的权值</p>
<h5 id="非线性前馈网络特点"><a href="#非线性前馈网络特点" class="headerlink" title="非线性前馈网络特点"></a>非线性前馈网络特点</h5><ul>
<li>同一层之间没有关系</li>
<li>不同层之间没有反馈（没有环路）</li>
<li>输入输出为离散值</li>
</ul>
<p>输入是随便的w，可以通过输入输出来调整w。</p>
<h5 id="修正方法："><a href="#修正方法：" class="headerlink" title="修正方法："></a>修正方法：</h5><img src="../../../img/image-20200729105442390.png" alt="image-20200729105442390" style="zoom:33%;">

<p>乘$$X_i$$的原因：输入的元素可能是负的，所以权值也要为负，因此需要乘起来</p>
<p>c表示学习率，不能太大（网络不稳定），不能太小（花费时间过多），适中即可（0.1～0.4）</p>
<h5 id="输入偏置"><a href="#输入偏置" class="headerlink" title="输入偏置"></a>输入偏置</h5><img src="../../../img/image-20200729105841928.png" alt="image-20200729105841928" style="zoom:33%;">

<p>偏置节点，权值表示为$$w_{0j}$$。</p>
<p>主要的应用场景是：假如输入全为0，可以根据调整偏置值来控制神经元是否激活 </p>
<h5 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h5><ol>
<li>权值初始化</li>
<li>输入样本对</li>
<li>计算输出</li>
<li>根据输出eg (1,0,0,0,1)调整权重</li>
<li>返回到步骤2输入到下一对样本，直至对所有样本的实际输出与期望输出相等</li>
</ol>
<img src="../../../img/image-20200729110553247.png" alt="image-20200729110553247" style="zoom:33%;">

<img src="../../../img/image-20200729110604651.png" alt="image-20200729110604651" style="zoom: 33%;">



<h5 id="例题：计算更新后的权重"><a href="#例题：计算更新后的权重" class="headerlink" title="例题：计算更新后的权重"></a>例题：计算更新后的权重</h5><p><img src="../../../img/image-20200729111046860.png" alt="image-20200729111046860" style="zoom:25%;">           <img src="../../../img/image-20200729112524006.png" alt="image-20200729112524006" style="zoom:33%;"></p>
<p><strong>方法</strong>：</p>
<p>更新权重方法：$$W_{i+1} = W_i + c (t - y) X_{i+1}$$</p>
<p>$$X_i$$ 表示为[$$x_0$$, $$x_1$$, $$x_2$$…]   （$$x_0$$一般取1）</p>
<p>通过样本学到分类器，分类器<strong>不唯一</strong>（学习参数，初始权值，学习样本的先后次序不同，最后的分类器可能不同，但分类效果相同）</p>
<h3 id="2-3-线性可分性"><a href="#2-3-线性可分性" class="headerlink" title="2.3 线性可分性"></a>2.3 线性可分性</h3><h5 id="OR函数"><a href="#OR函数" class="headerlink" title="OR函数"></a>OR函数</h5><p>ppt的练习题 我算出来是0.2, -0.02, 0.52（偏置单元是1）</p>
<p>书上也有例子 p57</p>
<h4 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h4><p>决策边界为 $$w^Tx+b=y$$（可能是直线，也可能是超平面）</p>
<p>也叫鉴别函数 $$xw^T &gt;=0$$</p>
<p>目标：找到一个权值矩阵，和x的点积。</p>
<p>物理解释：与x1，x2连线垂直的那条线 决策边界 $$w^Tx+b=y$$ 的法向量是 $$w$$</p>
<p>判断预测正确与否：$$y_i  (w^Tx+b) &gt; 0$$ （$$y_i$$表示标签，+/- 1）</p>
<blockquote>
<p>补充知识：</p>
<p>两向量夹角锐/直/钝 痛过点乘来判断</p>
<p>ax + by + C = 0 法向量为(a, b)</p>
<p>当$$w^Tx+b&gt;0$$在超平面法向量$$w$$所指的一侧，小于0表示在法向量的异侧。</p>
<img src="../../../img/image-20200729131848112.png" alt="image-20200729131848112" style="zoom: 25%;">
</blockquote>
<h5 id="多分类决策边界-多个输出-多分类"><a href="#多分类决策边界-多个输出-多分类" class="headerlink" title="多分类决策边界 - 多个输出 多分类"></a>多分类决策边界 - 多个输出 多分类</h5><p>每个输出神经元定义一个决策边界</p>
<p>疑问：上面的感知器模型就有多个输出神经元呀？？</p>
<h4 id="感知机收敛理论"><a href="#感知机收敛理论" class="headerlink" title="感知机收敛理论"></a>感知机收敛理论</h4><p>证明：给定一个线性可分数据集，感知机将在有限次迭代T后收敛到一个决策边界。 </p>
<p>定义γ是分离超平面与最接近的数据点之间的距离，则迭代次数的界是1/γ^2</p>
<p>证明方法：用$$w^<em>w$$来衡量，假设$$w^</em>$$可以线性可分（也就是能一个决策边界），因此需要找出尽可能和$$w^<em>$$平行的向量，也就是夹角为0，也就是让两者的点积越来越大。因此要考察两点，1是让$$w^</em>w$$变大，另外w长度没有增加太多</p>
<img src="../../../img/image-20200729212203704.png" alt="image-20200729212203704" style="zoom:33%;">

<p>结论：每次权重更新比上次都会增加γ，T步就会增加tγ</p>
<p>再利用柯西不等式</p>
<p><img src="../../../img/image-20200730000243478.png" alt="image-20200730000243478" style="zoom:33%;"><img src="../../../img/image-20200730000826070.png" alt="image-20200730000826070"></p>
<img src="../../../img/image-20200730000832394.png" alt="image-20200730000832394" style="zoom:33%;">



<h4 id="感知机缺点"><a href="#感知机缺点" class="headerlink" title="感知机缺点"></a>感知机缺点</h4><p>不能解决非线形可分的情况，如XOR。</p>
<p>解决办法：</p>
<ul>
<li><p>利用<strong>核技巧</strong>，投影到高维空间</p>
<ul>
<li>人为构造第三个维度</li>
</ul>
</li>
<li><p>用多层网络处理异或</p>
<ul>
<li><p>把单层感知机改为多层</p>
<p>权重是固定的</p>
<p>这个不算是感知机，是个感知器网络</p>
<img src="../../../img/image-20200730131118086.png" alt="image-20200730131118086" style="zoom:33%;">

<p>注：H1，H2也是两个神经元，因此他们的输出是根据激活函数来的，结果只能是0/1</p>
</li>
</ul>
</li>
</ul>
<h5 id="感知机的表达能力"><a href="#感知机的表达能力" class="headerlink" title="感知机的表达能力"></a>感知机的表达能力</h5><p>n维决策边界</p>
<p>❓广义布尔函数  pn个输入值至少有m个为真，则输出为真</p>
<p>两层神经网络可以表示所有的布尔函数</p>
<h5 id="感知机缺点-1"><a href="#感知机缺点-1" class="headerlink" title="感知机缺点"></a>感知机缺点</h5><p>感知机收敛的前提：$$w^*$$存在</p>
<p>如果线性不可分，只能收敛到最佳近似</p>
<h4 id="感知机方法只适用在单层网络！！"><a href="#感知机方法只适用在单层网络！！" class="headerlink" title="感知机方法只适用在单层网络！！"></a>感知机方法只适用在单层网络！！</h4><h1 id="3-神经元网络"><a href="#3-神经元网络" class="headerlink" title="3. 神经元网络"></a>3. 神经元网络</h1><h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>使用更复杂的网络，两个方法：</p>
<ul>
<li>加入向后的连接，输入输出能连接起来（造成循环网络）</li>
<li>在输入输出之间加入神经元</li>
</ul>
<p>隐层结点是全连接</p>
<h5 id="隐层神经元"><a href="#隐层神经元" class="headerlink" title="隐层神经元"></a>隐层神经元</h5><p>隐藏层叫法是因为不能直接检查并修正它们的值</p>
<h5 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h5><p>通过权值，对输入数据做一次映射，实际上：特征检测算子，刻画训练数据的突出特征</p>
<img src="../../../img/image-20200225164240825.png" alt="image-20200225164240825" style="zoom:33%;">

<p>将四个数据压缩到三个（上图中的两行数据进行压缩）</p>
<p>对输入数据做一次<strong>特征变化</strong>(乘w)，再把新的数据再到输出层进行线性分割</p>
<blockquote>
<p>隐藏层神经元实际为特征检测算子(feature detector)，在多层神经网络的学习过程中，隐藏层神经元开始逐步“发现”刻画训练数据的突出特征</p>
</blockquote>
<h5 id="权值的计算方法："><a href="#权值的计算方法：" class="headerlink" title="权值的计算方法："></a>权值的计算方法：</h5><p>工作机制：向前 向后</p>
<p>向前：逐层计算输入输出，最后计算误差。这个也叫算法的recall 阶段。</p>
<p>向后（回头看）：通过输出层输出的误差来反向计算权值，逐层调整权值。属于梯度下降的一种形式。</p>
<h4 id="误差反传（BP，back-propapagation"><a href="#误差反传（BP，back-propapagation" class="headerlink" title="误差反传（BP，back-propapagation)"></a>误差反传（BP，back-propapagation)</h4><h5 id="误差定义："><a href="#误差定义：" class="headerlink" title="误差定义："></a>误差定义：</h5><ul>
<li><p>感知器</p>
<img src="../../../img/image-20200225164654739.png" alt="image-20200225164654739" style="zoom:25%;">

<p>注：经典感知机中N = 1。当有多个输出神经元的时候，如果误差一正一负，可能就抵消了。因此选择<strong>平方和误差函数</strong>。</p>
</li>
<li><p>多层感知机</p>
<p><img src="../../../img/image-20200225164730273.png" alt="image-20200225164730273" style="zoom:25%;"> 【1/2并非必须，便于求导约分】</p>
<p>沿着误差梯度下降的话，误差下降最快</p>
<img src="../../../img/image-20200731091428367.png" alt="image-20200731091428367" style="zoom:25%;">

</li>
</ul>
<h5 id="Delata规则："><a href="#Delata规则：" class="headerlink" title="Delata规则："></a>Delata规则：</h5><p>找误差最小的那个点。</p>
<img src="../../../img/image-20200731093015963.png" alt="image-20200731093015963" style="zoom:33%;">

<p>c表示学习步长</p>
<p>误差平面指的是函数在数据集上的累积误差。每一个神经网络的权值向量表示误差平面的一个点。</p>
<p>要求：<strong>激励函数必须连续可微分</strong>。（跃阶函数不连续，sigmoid函数可用）</p>
<p>不能克服缺点（只能线性分类的缺点），但它是BP的核心。</p>
<p>TODO 推导可以再看看</p>
<p>ppt 15  课本p120</p>
<p>两步：</p>
<ol>
<li><p>先计算Error对$$O_i$$的偏导</p>
<img src="../../../img/image-20200731133743523.png" alt="image-20200731133743523" style="zoom:25%;">
</li>
<li><p>计算$$\Delta W_k$$</p>
</li>
</ol>
<img src="../../../img/image-20200731133547262.png" alt="image-20200731133547262" style="zoom:33%;">

<p>比感知器多了<strong>中间激活函数导数</strong>这项（感知机阶跃函数不能求导）</p>
<p>注意：</p>
<p>c越大，权值朝最优值的速度越快。如果过大在最优值附近会震荡。</p>
<p>delta规则不能克服感知机的界限。但他是BP的核心（而Hebb规则做不到）原因就是他的可导性</p>
<h5 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h5><p>把输出层的误差往前做传播</p>
<p>信用分配问题：只知道输出，但不知道中间隐藏结点的输出。</p>
<h3 id="多层感知机-→-BP神经网络"><a href="#多层感知机-→-BP神经网络" class="headerlink" title="多层感知机 → BP神经网络"></a>多层感知机 → BP神经网络</h3><p>BP神经网络的<u>本质就是多层感知机</u></p>
<p>特点：</p>
<ul>
<li><p>三层或三层以上结构（输入，隐藏，输出）</p>
</li>
<li><p>无反馈（反传是误差，反馈是输入输出）</p>
</li>
<li><p>层内无互联（RNN，LSTM是在同一层内联系）</p>
</li>
<li><p>采用误差反向传播算法</p>
</li>
</ul>
<h5 id="【重要】BP误差反差学习传播算法推导"><a href="#【重要】BP误差反差学习传播算法推导" class="headerlink" title="【重要】BP误差反差学习传播算法推导"></a>【重要】BP误差反差学习传播算法推导</h5><p>教材p88</p>
<blockquote>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html" target="_blank" rel="noopener">https://www.cnblogs.com/charlotte77/p/5629865.html</a> 带了数字计算</p>
</blockquote>
<img src="../../../img/image-20200731135733939.png" alt="image-20200731135733939" style="zoom:33%;">

<p>两种情况：</p>
<h5 id="无隐藏层："><a href="#无隐藏层：" class="headerlink" title="无隐藏层："></a>无隐藏层：</h5><p>图见上方</p>
<p><img src="../../../img/image-20200731172436071.png" alt="image-20200731172436071" style="zoom:25%;"><img src="../../../img/image-20200731172641157.png" alt="image-20200731172641157" style="zoom:25%;"></p>
<p>要计算某个权值对整个误差造成的影响，所以要计算上面这个。</p>
<p>局域梯度域（能复用）为：<img src="../../../img/image-20200731172553351.png" alt="image-20200731172553351" style="zoom:33%;"></p>
<p>最终结果 <img src="../../../img/image-20200731172613572.png" alt="image-20200731172613572" style="zoom:25%;"></p>
<h5 id="有隐藏层："><a href="#有隐藏层：" class="headerlink" title="有隐藏层："></a>有隐藏层：</h5><p>❓这里的权值 $$w_{kj}$$表示j到k，但是这里明明只有一个神经元j？</p>
<img src="../../../img/image-20200731172402617.png" alt="image-20200731172402617" style="zoom:25%;">

<p>信用分配问题：隐藏神经元不能直接访问，因为隐藏结点的输出对后面的误差都有贡献</p>
<p>要算隐藏层的权重的话，需要先计算到$$y_j$$的偏导</p>
<img src="../../../img/image-20200731174054737.png" alt="image-20200731174054737" style="zoom:33%;">

<img src="../../../img/image-20200731174612330.png" alt="image-20200731174612330" style="zoom:33%;">

<p>（因为$$y_j$$对每个$$e_k$$都有贡献）</p>
<p>注意：网络中隐藏层和输出层中用的激励函数是否相同，对于推导结果没有影响。</p>
<img src="../../../img/image-20200731174816778.png" alt="image-20200731174816778" style="zoom:25%;">

<img src="../../../img/image-20200731174829947.png" alt="image-20200731174829947" style="zoom:25%;">

<p>❓有啥物理意义呢</p>
<img src="../../../img/image-20200731184738401.png" alt="image-20200731184738401" style="zoom:25%;">

<blockquote>
<p>例题：<a href="https://blog.csdn.net/dare_kz/article/details/77603522" target="_blank" rel="noopener">https://blog.csdn.net/dare_kz/article/details/77603522</a></p>
</blockquote>
<h5 id="sigmoid导数"><a href="#sigmoid导数" class="headerlink" title="sigmoid导数"></a>sigmoid导数</h5><p>$$f(net) = \frac{1}{1+e^{-\lambda*net}}$$</p>
<p>特点：$$\lambda$$越大，区间[0,1]上越接近直线</p>
<p>求导结果：$$-λf(net)(1-f(net))$$</p>
<h5 id="其他议题"><a href="#其他议题" class="headerlink" title="其他议题"></a>其他议题</h5><ul>
<li><p>初始权值</p>
<p>初始权值设定为[0, 1]中的高斯分布中选一个</p>
<p>假设数据样本均匀分布，和高斯分布的权值相乘就会产生随机变量。那么隐藏层也会变成随机变量。</p>
</li>
</ul>
<p>  独立变量和的方差 = 独立变量方差的和 如果n个权值，输出值~(0, n)。意味着高斯分布很平整</p>
<p>  这就意味着输出层的输入会是n这样的大值，导致输出值过大，进一步导致激活函数饱和，梯度趋近于0，学习失效。</p>
<p>  因此权值应该$$w~(0, \frac{1}{n^{\frac{1}{2}}})$$    n指的是输入层结点数</p>
<ul>
<li><p>激活函数</p>
<p>分类：sigmoid</p>
<p>回归：线性函数 y = h</p>
<p>多分类：softmax函数</p>
<p>深度学习用非饱和激活函数</p>
</li>
<li><p>顺序和批量训练</p>
<ul>
<li><p>批量训练（所有训练样本的误差和）</p>
<p>计算代价大/收敛速度快/局部极小</p>
</li>
<li><p>顺序训练</p>
<p>按顺序挨个计算。快/局部极小/噪声敏感</p>
</li>
<li><p>小批量训练（mini batch）</p>
<p>从中随机挑几个进行训练（有的样本可能挑多次）</p>
<p>适用于大规模数据</p>
</li>
</ul>
</li>
<li><p>局部极小和冲量</p>
<p>梯度为0可能是局部极小，冲力就是意味着给小球额外重量，使它产生滚动的动量。这个通过对当前点的前一个权重进行改变</p>
<p>更新的时候顺便计算二阶导</p>
<img src="../../../img/image-20200801005013602.png" alt="image-20200801005013602" style="zoom:25%;">

<p>a是冲力常量</p>
</li>
<li><p>停止机制</p>
<ul>
<li>固定迭代步数</li>
<li>误差小于固定阈值（如果有噪声可能停不了，因此不选等于）</li>
<li>结合</li>
</ul>
<p>利用验证集观察误差变化，找到合适参数。</p>
<p>（先让他跑，不设定停止条件，找出误差变化规律。误差增大可能就过拟合了）</p>
</li>
</ul>
<h3 id="自动编码器AUTOEncoder-最早深度学习的起源"><a href="#自动编码器AUTOEncoder-最早深度学习的起源" class="headerlink" title="自动编码器AUTOEncoder - 最早深度学习的起源"></a>自动编码器AUTOEncoder - 最早深度学习的起源</h3><p>无监督算法，目标值和输入值相等，相当于学习函数h(x)≈x。</p>
<img src="../../../img/image-20200801090553441.png" alt="image-20200801090553441" style="zoom:25%;">

<p>也就是要学到W 通过W得到低维的输出</p>
<p>相当于中间做的是数据的压缩</p>
<p>层数变多</p>
<p>顶层MLP的三层结构</p>
<p>输入 - 隐藏 - 输出</p>
<p>网络是对称的（输入结点数 = 输出结点数，隐层结点&lt; 输入/出结点）</p>
<p>MLP中的训练样本有label，自动编码器的数据没有类标。【<strong>自监督学习</strong>】</p>
<img src="https://tva1.sinaimg.cn/large/0082zybply1gcb11yagxhj30xy0c441k.jpg" alt="image-20200227161401432" style="zoom:25%;">

<blockquote>
<p> 编码权值</p>
<p>解码权值                                                                    </p>
<p>找到两个网络参数，使得经过变换后误差最小。</p>
<p>输入、输出是<strong>转置</strong>的关系。</p>
</blockquote>
<p>只需要解一个W’</p>
<p>要求中间的输出尽可能稀疏（如稀疏约束）</p>
<h3 id="径向基网络RBF-有监督学习"><a href="#径向基网络RBF-有监督学习" class="headerlink" title="径向基网络RBF - 有监督学习"></a>径向基网络RBF - 有监督学习</h3><h5 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h5><p>感受野越大表示能接触到原始图像的范围越大，也意味着他可能蕴含更为全局、语义层次更高的特征；而值越小则表示其所包含的特征越趋向于局部和细节。</p>
<h5 id="径向基网络"><a href="#径向基网络" class="headerlink" title="径向基网络"></a>径向基网络</h5><p>目的：相近的输入产生相同的输出，相隔很远的输入不会产生相同的输出</p>
<p>RBF神经元的激活和<strong>输入数据</strong>和<strong>权重空间中神经元的距离</strong>成比例</p>
<p>用输入与中心向量的欧式距离作为函数自变量，高斯函数作为激活函数（空间中任一点x到某一中心c之间欧氏距离的单调函数）</p>
<p>x 光强 w 锥状体</p>
<p>x在这样一个高斯函数上，是否会有输出，输出是多少</p>
<p>中心点：径向基位置</p>
<p>圆圈大小：感受野尺寸</p>
<img src="../../../img/image-20200801215109908.png" alt="image-20200801215109908" style="zoom:33%;">



<p>基：表示激活/未被激活的锥状体</p>
<p>RBF：是否被激活：基于<strong>距离</strong>，局部匹配。不是每个都有输出。</p>
<p>（MLP：基于<strong>内积</strong>，全局匹配，因为全连接，隐层都会被激活）</p>
<h5 id="径向基网络的学习"><a href="#径向基网络的学习" class="headerlink" title="径向基网络的学习"></a>径向基网络的学习</h5><p>需要计算的三个参数：基函数的中心，方差，隐含层到输出层的权值</p>
<p>因为隐藏层的输出可以被计算出来，因此RBF退化为单层网络</p>
<blockquote>
<p><img src="../../../img/image-20200801232438858.png" alt="image-20200801232438858"></p>
<p> Ps: 第一步有两种方法：</p>
<ul>
<li>如图中所提到的均值算法</li>
<li>随机初始化的数据点作为中心（感知机）</li>
</ul>
<img src="../../../img/image-20200801231802964.png" alt="image-20200801231802964" style="zoom:33%;">
</blockquote>
<h5 id="径向基网络的原理"><a href="#径向基网络的原理" class="headerlink" title="径向基网络的原理"></a>径向基网络的原理</h5><ul>
<li><p>基构成隐层空间，通过确定RBF的中心点，把输入直接映射到隐空间，不需要权连接</p>
</li>
<li><p>隐层的输出是线性的，网络的输出就是线性加权和</p>
</li>
<li><p>隐含层的作用是从低维度映射到高维度。把低维线性不可分到高维线性可分。【核函数思想】</p>
<ul>
<li>隐藏层结点比输入层多（自编码器隐藏层结点比输入层少，起到降维的效果）</li>
</ul>
</li>
<li><p>优点：</p>
<ul>
<li>快，权值能直接计算不需要迭代，能避免局部最小问题（前提：基需要先固定下来，尺寸大小是什么）</li>
</ul>
</li>
</ul>
<h5 id="RBF-vs-BP"><a href="#RBF-vs-BP" class="headerlink" title="RBF vs BP"></a>RBF vs BP</h5><ul>
<li><p>局部逼近 vs 全局逼近</p>
<p><strong>BP神经网络</strong>：隐藏层对输入层做了全连接。是对非线性映射的<strong>全局逼近</strong>。所有的输入都用到了权值</p>
<p><strong>RBF 局部映射</strong>：局部映射</p>
</li>
<li><p>中间层数区别</p>
<p>BP 可以有多个隐藏层（如果层数太多，计算量太大，梯度发散，梯度消失）</p>
<p>RBF 只有一个</p>
</li>
<li><p>训练速度</p>
<p>RBF快：</p>
<ul>
<li>隐含层少</li>
<li>对于一个输入x，只有部分神经元会有响应，其他的都近似为0，对应的w就不用调参了</li>
</ul>
</li>
<li><p>最优性</p>
<p>RBF是连续函数的最佳逼近</p>
</li>
</ul>
<p>注：如果用学习到的算法学输出层的值，那还是得用到BP算法</p>
<p>局部的拟合器 组合起来 效果是最佳的</p>
<blockquote>
<p><a href="https://www.cnblogs.com/pinking/p/9349695.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinking/p/9349695.html</a></p>
<img src="../../../img/image-20200801231826526.png" alt="image-20200801231826526" style="zoom:33%;">
</blockquote>
<h5 id="RBF-vs-SVM"><a href="#RBF-vs-SVM" class="headerlink" title="RBF vs SVM"></a>RBF vs SVM</h5><p><img src="../../../img/image-20200801235809096.png" alt="image-20200801235809096"></p>
<h1 id="4-维度约减（降维）"><a href="#4-维度约减（降维）" class="headerlink" title="4. 维度约减（降维）"></a>4. 维度约减（降维）</h1><h3 id="特征选择和降维"><a href="#特征选择和降维" class="headerlink" title="特征选择和降维"></a>特征选择和降维</h3><h4 id="降维："><a href="#降维：" class="headerlink" title="降维："></a>降维：</h4><ul>
<li><p>特征选择：维度约减选择的特征是原来特征的子集</p>
</li>
<li><p>特征诱导/<strong>变化</strong>：通过数据产生了新特征（比如深度学习又是一种表示学习，通过数据自动学出原来数据没有的特征。早期的实验是通过数据，构造出来一些特征，然后再做特征诱导，而深度学习是直接学特征，有端到端的优点）</p>
</li>
</ul>
<h5 id="目的：-1"><a href="#目的：-1" class="headerlink" title="目的："></a>目的：</h5><ul>
<li>降维，降低over-fitting风险（低维空间下，数据量足够，就不会over-fiting了）<ul>
<li>overfitting有两种方法，一种是降维，另一种是增加样本</li>
</ul>
</li>
<li>增加解释性（低维度下解释性比较强，能做到可视化）</li>
<li>去除冗余特征</li>
</ul>
<h4 id="特征选择（这章不讲）"><a href="#特征选择（这章不讲）" class="headerlink" title="特征选择（这章不讲）"></a>特征选择（这章不讲）</h4><p>数据的特征之间是否有依赖性？</p>
<p>本质上是<strong>搜索</strong>问题，就是要搜一个最优的特征子集（也就是要构建搜索树）</p>
<p>特征评估函数</p>
<ul>
<li><p>过滤式</p>
<p>增强特征和类的相关性，削减特征之间的相关性</p>
<p>距离度量（方差为0，可以过滤掉这个特征）、信息度量（互信息熵）、</p>
</li>
<li><p>封装式</p>
<p>把特征选择和机器学习任务目标放在一个框架</p>
<p>通过分类错误率指导特征选择（对最终任务目标提升的好坏）</p>
</li>
<li><p>嵌入式</p>
<p>模型正则化，再加上稀疏约束</p>
</li>
</ul>
<h3 id="LDA-线性判别分析-监督学习"><a href="#LDA-线性判别分析-监督学习" class="headerlink" title="LDA 线性判别分析 - 监督学习"></a>LDA 线性判别分析 - 监督学习</h3><p>ps：监督学习是因为一开始有类别标签，才能算类内类间散度</p>
<blockquote>
<p>数学知识：</p>
<p><img src="../../../img/image-20200803105307581.png" alt="image-20200803105307581"></p>
</blockquote>
<p>投影到超平面，让样本按照类别最大程度分开（投影平面和分类平面垂直）</p>
<p>$$S_W$$(Within-class)组内方差 越小越好  <img src="../../../img/image-20200803083253301.png" alt="image-20200803083253301" style="zoom: 33%;"></p>
<p>$$S_B$$(Between-class)组间方差 越大越好  <img src="../../../img/image-20200803083309294.png" alt="image-20200803083309294" style="zoom:33%;"></p>
<p>目标是$$\frac{S_W}{S_B}$$最小化，然后用$$w^T$$将$$x_j$$转为在直线$$w$$上的投影，也就是用$$w^Tx_j$$代替$$x_j$$</p>
<p>目标函数变为$$\frac{w^TS_Ww}{w^TS_Bw}$$</p>
<p>然后对目标函数中的w求导，令导数为0</p>
<img src="../../../img/image-20200803101759743.png" alt="image-20200803101759743" style="zoom:33%;">

<p>要求解这个，最开始思路是左乘$$S_W^{-1}$$ ，但是通常情况下$$S_W^{-1}$$无法计算，所以需要计算$$S_W^{-1}S_B$$的特征值和特征向量</p>
<h5 id="算法基本流程"><a href="#算法基本流程" class="headerlink" title="算法基本流程"></a>算法基本流程</h5><blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6244265.html</a></p>
</blockquote>
<p>设d为要降到的维度</p>
<ul>
<li>计算每个类别的均值，和全局样本均值</li>
<li>计算组间方差$$S_B$$和组内方差$$S_W$$ （聚类中的指标）</li>
<li>对矩阵$$S_W^{-1}S_B$$做特征值分解</li>
<li>取最大的d个特征值对应的特征向量</li>
<li>计算投影矩阵</li>
</ul>
<h3 id="PCA-主成分分析-无监督"><a href="#PCA-主成分分析-无监督" class="headerlink" title="PCA 主成分分析 - 无监督"></a>PCA 主成分分析 - 无监督</h3><blockquote>
<p>数学知识：</p>
<p><a href="https://zhuanlan.zhihu.com/p/77151308" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/77151308</a> 这篇相关数学知识讲的好</p>
<p>矩阵的主成分是其协方差矩阵的特征向量按照对应的特征值大小排序得到的</p>
<p>样本比较大的时候，协方差公式里的m-1可以直接用m来代替</p>
<p>协方差矩阵对角线是单个变量方差，非对角线是协方差</p>
<p>降维问题的优化目标：<strong>将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0（因为不相关），而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）</strong></p>
</blockquote>
<p>可以把数据映射到主轴方向</p>
<p>找到数据中的主要成分，并用它来表征数据。</p>
<p>降维是因为取出了几个特征值最大的维度（不然只是做了PCA，只做了坐标轴变换）</p>
<h5 id="主成分特点"><a href="#主成分特点" class="headerlink" title="主成分特点"></a>主成分特点</h5><ul>
<li>最大可分性  样本点在第一主成分的<strong>离散程度</strong>大于第二主成分（方差越大越能保留更多的信息，都聚一块那信息都丢失了）<ul>
<li>从数学上来说就是要找k个基(k &lt; N)，还要最大程度保留信息</li>
</ul>
</li>
<li>最近可重构性 样本点到第一主成分的平均距离小于到第二主成分距离</li>
</ul>
<h5 id="目标函数：最大化样本点在主成分投影上的方差"><a href="#目标函数：最大化样本点在主成分投影上的方差" class="headerlink" title="目标函数：最大化样本点在主成分投影上的方差"></a><strong>目标函数</strong>：最大化样本点在主成分投影上的方差</h5><h5 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h5><blockquote>
<p><a href="https://www.cnblogs.com/xbinworld/archive/2011/11/24/pca.html" target="_blank" rel="noopener">https://www.cnblogs.com/xbinworld/archive/2011/11/24/pca.html</a></p>
<p>数学细节：</p>
<p>对随机变量来说，cov(X, Y) = E[(X-u)(Y-u)]  $$cov(W^TX)=\frac{1}{m}(X-\mu)(X-\mu)^T$$</p>
</blockquote>
<p>定义W为包含所有映射向量为列向量的矩阵</p>
<p>投影后的数据 $$x_i^TW$$： 去中心化数据后做投影，还是以0为中心</p>
<p>投影后数据方差：<img src="../../../img/image-20200803142634934.png" alt="image-20200803142634934" style="zoom:33%;"></p>
<p>目标是让投影后方差最大，所以就是要让协方差矩阵对角线上的迹取到$$max(tr(W^TCW))$$，并且$$W^TW=I$$(因为是正交投影)</p>
<p>ps: C表示协方差矩阵</p>
<p>最优的W是由数据协方差矩阵前<em>k</em>个最大的特征值对应的特征向量作为列向量</p>
<p>构成的PCA的输出就是Y = W’X，由X的原始维度降低到了k维</p>
<h5 id="拉格朗日乘子法（用来证明要优化的目标）："><a href="#拉格朗日乘子法（用来证明要优化的目标）：" class="headerlink" title="拉格朗日乘子法（用来证明要优化的目标）："></a>拉格朗日乘子法（用来证明要优化的目标）：</h5><p>也可以直接用实对称矩阵的性质推到</p>
<img src="../../../img/image-20200803143130672.png" alt="image-20200803143130672" style="zoom:33%;">

<p>结论：x 投影后的方差就是协方差矩阵的特征值。我们要找到最大方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量，次佳就是第二大特征值对应的特征向量，以此类推。</p>
<h5 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h5><ul>
<li>样本去中心化 （每个数据都减掉样本均值，这样均值为0，所以协方差矩阵为$$xx^T$$）</li>
<li>计算协方差矩阵 $$C = \frac{1}{m}(x-\mu)(x-\mu)^T = \frac{1}{m}xx^T $$</li>
<li>对协方差矩阵做特征值分解</li>
<li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 </li>
<li>Y = WX即为降维到 k 维后的数据</li>
</ul>
<h5 id="核PCA"><a href="#核PCA" class="headerlink" title="核PCA"></a>核PCA</h5><p>对于呈现环状的点分布，找不到线性变换正交坐标轴</p>
<p>思路：用类标让他们在投影中区分出来，也就是核PCA</p>
<h5 id="PCA-vs-LDA"><a href="#PCA-vs-LDA" class="headerlink" title="PCA vs LDA"></a>PCA vs LDA</h5><table>
<thead>
<tr>
<th>PCA</th>
<th>LDA</th>
</tr>
</thead>
<tbody><tr>
<td>无监督</td>
<td>有监督</td>
</tr>
<tr>
<td>方差最大</td>
<td>SB最大，SW最小</td>
</tr>
<tr>
<td>投影后的坐标是正交的（因为选择的成分都是正交的，也就是说协方差矩阵非对角线都是0）</td>
<td>非正交</td>
</tr>
<tr>
<td>投影后维度数与源数据相同</td>
<td>投影后维度数目与类别数目相同</td>
</tr>
</tbody></table>
<h3 id="ICA-独立成分分析"><a href="#ICA-独立成分分析" class="headerlink" title="ICA 独立成分分析"></a>ICA 独立成分分析</h3><h5 id="因素分析"><a href="#因素分析" class="headerlink" title="因素分析"></a>因素分析</h5><p>数据由多个物理源产生，因素分析将数据解释为<strong>不相关因素的累加</strong></p>
<p>目标是找到数据源</p>
<p>做去中心化的预处理</p>
<p>目标模型是$$X = WY + \epsilon$$， 因素Y做了线性变换W，现在就是想知道W和Y分别是什么。$$\epsilon$$是噪声</p>
<p>模型限制：（加入限制才能可解)</p>
<ul>
<li><p>数据之间不相关，协方差为0</p>
</li>
<li><img src="../../../img/image-20200804095430039.png" alt="image-20200804095430039" style="zoom:33%;">

<blockquote>
<p>❓ 最后一步没看懂</p>
</blockquote>
</li>
</ul>
<p>然后用EM算法求解</p>
<h5 id="盲源分离（源未知）"><a href="#盲源分离（源未知）" class="headerlink" title="盲源分离（源未知）"></a>盲源分离（源未知）</h5><p>数据来源于一些<strong>独立</strong>的潜在物理过程</p>
<p>不相关：$$cov(b_i, b_j) = 0$$</p>
<p>统计独立：$$E(b_i,b_j) = E(b_i)(b_j)$$ 更强的约束，满足统计独立，那肯定不相关</p>
<h5 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h5><p>$$x1 = as_1+bs_2$$</p>
<p>$$x2 = cs_1+ds_2$$</p>
<p>$$x = As$$  A表示线性变换，s表示原数据源，x表示产生的输入声音</p>
<p>按照一般思路求s，但A未知，所以不能求，需要加入约束：</p>
<ul>
<li>数据源独立，混合数据不独立</li>
<li>数据源非高斯变量，混合数据服从高斯分布（中心极限定理）</li>
</ul>
<p>$$y = W^Tx = W^T As = z^Ts$$</p>
<p>当z只有一项非0元素，<strong>y的非高斯性最大</strong>(意味着只保留了混合数据的一项，这样非高斯性最大)</p>
<p>目标函数：</p>
<p>熵表示了y的随机性，高斯性</p>
<p>负熵：$$J(y) = H(z) - H(y)$$</p>
<p>近似方法：$$J(y) = (E[G(y)]-E[G(x)])^2$$</p>
<blockquote>
<p>所有等方差的随机变量中，高斯变量的熵最大。由中心极限定理，若干个有限方差随机变量（无论其服从何种分布）的和，其逼近高斯分布。反言之，<strong>源信号比混合信号的非高斯性更</strong>强。用负熵度量其非高斯性。</p>
</blockquote>
<h5 id="主要算法"><a href="#主要算法" class="headerlink" title="主要算法"></a>主要算法</h5><ul>
<li><p>infoMax</p>
</li>
<li><p>FastICA（固定点算法，寻求 X 分量在 W 上投影$$w^Tx$$ 的非高斯最大化，）</p>
<p>目标：要找到一个最优方向 w ，使得该方向的非高斯性最大</p>
<blockquote>
<p><a href="http://skyhigh233.com/blog/2017/04/01/ica-math/" target="_blank" rel="noopener">http://skyhigh233.com/blog/2017/04/01/ica-math/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/133511600" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/133511600</a></p>
</blockquote>
</li>
</ul>
<img src="../../../img/image-20200804174608345.png" alt="image-20200804174608345" style="zoom:33%;">

<h5 id="ICA-vs-PCA"><a href="#ICA-vs-PCA" class="headerlink" title="ICA vs PCA"></a>ICA vs PCA</h5><table>
<thead>
<tr>
<th>PCA</th>
<th>ICA</th>
</tr>
</thead>
<tbody><tr>
<td>降维并提取不相关属性</td>
<td>降维并提取相互独立属性</td>
</tr>
<tr>
<td>重构误差最小</td>
<td>每个分量最大化独立</td>
</tr>
<tr>
<td>成分间正交</td>
<td>成分间独立</td>
</tr>
<tr>
<td>信息提取</td>
<td>解混</td>
</tr>
</tbody></table>
<h3 id="局部线性嵌入-LLE"><a href="#局部线性嵌入-LLE" class="headerlink" title="局部线性嵌入 LLE"></a>局部线性嵌入 LLE</h3><p>局部最优，在降维时能保持样本的局部特征</p>
<blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6266408.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6266408.html</a></p>
<p><img src="../../../img/image-20200805083003640.png" alt="image-20200805083003640"></p>
</blockquote>
<h5 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h5><ul>
<li><p>投射到新空间后，要保留相邻数据的关系（保持原有的结构）</p>
</li>
<li><p>数据用局部近邻线性近似（自身可以用近邻点表达出来）</p>
</li>
</ul>
<img src="../../../img/image-20200805082616888.png" alt="image-20200805082616888" style="zoom: 33%;">

<h5 id="近邻点的确定"><a href="#近邻点的确定" class="headerlink" title="近邻点的确定"></a>近邻点的确定</h5><ul>
<li>距离法：距离小于阈值 </li>
<li>个数法：前k个</li>
</ul>
<h5 id="权重约束"><a href="#权重约束" class="headerlink" title="权重约束"></a>权重约束</h5><ul>
<li>$$x_i$$周围点的权重和为1，</li>
<li>如果某个点离$$x_i$$很远，那么$$W_{ij} = 0$$</li>
</ul>
<h5 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h5><img src="../../../img/image-20200805084356732.png" alt="image-20200805084356732" style="zoom:33%;">

<img src="../../../img/image-20200805085925761.png" alt="image-20200805085925761" style="zoom:50%;">



<h3 id="ISOMAP-等距特征映射"><a href="#ISOMAP-等距特征映射" class="headerlink" title="ISOMAP - 等距特征映射"></a>ISOMAP - 等距特征映射</h3><p>目标：保留相邻数据的关系</p>
<p>方法：检查所有点对的距离 &amp; 计算全局测地线</p>
<h4 id="MDS-多维缩放"><a href="#MDS-多维缩放" class="headerlink" title="MDS 多维缩放"></a>MDS 多维缩放</h4><blockquote>
<p><a href="https://blog.csdn.net/Dark_Scope/article/details/53229427" target="_blank" rel="noopener">https://blog.csdn.net/Dark_Scope/article/details/53229427</a></p>
</blockquote>
<p>ISOMAP = MDS + 测地线</p>
<p>寻找线性近似，然后投射到低维空间（类似PCA）</p>
<p>嵌入时要保持数据点对之间的距离（假定距离已经被测出）</p>
<p>欧式空间，PCA和MDS等价</p>
<blockquote>
<p>已知高维上样本点两两之间的距离，尝试在低维上(通常是2维，但是可以是任意维)找到一组新的样本点，使降维后两点间的距离与它们在高维上的距离相等</p>
</blockquote>
<h5 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h5><img src="../../../img/image-20200805092303236.png" alt="image-20200805092303236" style="zoom:33%;">

<p>Sammon中那个除法是为了对误差加权（举例理解：同宿舍和同楼层都属于近邻，但距离远的应该除以初始距离）</p>
<p>Sammon是非线性的，只能用迭代来做</p>
<h5 id="经典MDS算法"><a href="#经典MDS算法" class="headerlink" title="经典MDS算法"></a>经典MDS算法</h5><p>去中心化后用相似度代替距离</p>
<p>相似度：<img src="../../../img/image-20200805092636404.png" alt="image-20200805092636404" style="zoom:33%;"></p>
<p>目标函数：</p>
<img src="../../../img/image-20200805092747415.png" alt="image-20200805092747415" style="zoom:33%;">

<p>解法：</p>
<ul>
<li>梯度下降</li>
<li>当目标函数是线性，直接解析计算（求导求逆）</li>
</ul>
<p>算法流程：</p>
<img src="../../../img/image-20200813093018635.png" alt="image-20200813093018635" style="zoom:33%;">



<h5 id="流形空间"><a href="#流形空间" class="headerlink" title="流形空间"></a>流形空间</h5><p>流形：任何对象都可以看作低维在高维空间的嵌入（表达）</p>
<p>毛巾是二维，扭转后是三维</p>
<h5 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h5><p>球上两点的距离是测地距离，不是欧式距离</p>
<h5 id="测地线距离"><a href="#测地线距离" class="headerlink" title="测地线距离"></a>测地线距离</h5><img src="../../../img/image-20200805094035538.png" alt="image-20200805094035538" style="zoom:25%;">

<p>表达高维空间的距离（如AB间距离），用欧式距离不合适</p>
<p>计算测地线距离的方法：</p>
<p>根据近邻不断的寻找，近邻是用欧式距离，然后接着再找近邻的近邻。。。</p>
<ul>
<li>Dijstra算法</li>
<li>BFS</li>
</ul>
<h5 id="ISOMAP算法"><a href="#ISOMAP算法" class="headerlink" title="ISOMAP算法"></a>ISOMAP算法</h5><ul>
<li>创建所有点对之间的欧式距离</li>
<li>找到每个点的近邻点</li>
<li>通过找最短路径法计算测地线距离$$d_G$$</li>
<li>对$$d_G$$运用MDS算法</li>
</ul>
<h1 id="7-优化与搜索"><a href="#7-优化与搜索" class="headerlink" title="7 优化与搜索"></a>7 优化与搜索</h1><p>在机器学习之类的实际应用中，我们一般将最优化问题统一表述为求解函数的极小值问题，</p>
<h5 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h5><p>梯度：矢量，方向指向数值增长最快的方向，大小为变化率。</p>
<p>导数：一个值</p>
<p>确定目标函数是关键</p>
<p>求解目标函数：</p>
<ul>
<li>对问题直接进行计算</li>
<li>凸优化，非凸优化的技术来求解</li>
</ul>
<p>优化方法很重要</p>
<h3 id="梯度下降-寻找导数为0的点"><a href="#梯度下降-寻找导数为0的点" class="headerlink" title="梯度下降 - 寻找导数为0的点"></a>梯度下降 - 寻找导数为0的点</h3><h5 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h5><p>迭代法</p>
<p>找目标函数的最小值，但解析法不能直接求出</p>
<ul>
<li><p>基于学习率的改进机制</p>
<p>学习率适应梯度（梯度大的时候，学习率应该小）</p>
<p>学习率 除以 梯度（平方开根保证正）</p>
</li>
<li><p>基于二阶梯度的改进机制</p>
<p>利用之前的梯度指导梯度下降</p>
<p>（避免被局部最优影响，找二阶梯度 &amp; 一阶梯度都大的地方，之前一阶梯度的方向得到保证，继续沿原来的方向走）</p>
<p>冲量法</p>
</li>
</ul>
<h4 id="代表算法"><a href="#代表算法" class="headerlink" title="代表算法"></a>代表算法</h4><h5 id="冲量法"><a href="#冲量法" class="headerlink" title="冲量法"></a>冲量法</h5><p>将当本次梯度下降方向与上次更新量的方向相同时，上次的更新量能够对本次的搜索起到一个正向加速的作用；反之减速。</p>
<p>借助变量$$V_t$$更新的时候同时更新$$\Delta W_t$$和以前的$$\Delta W_{t-1}$$…</p>
<p>指数平均距离</p>
<h5 id="NAG"><a href="#NAG" class="headerlink" title="NAG"></a>NAG</h5><p>把值和方向都考虑进来</p>
<p>根据$$W_*$$的方向来走</p>
<img src="../../../img/image-20200806112634271.png" alt="image-20200806112634271" style="zoom: 25%;">

<h5 id="自适应梯度法"><a href="#自适应梯度法" class="headerlink" title="自适应梯度法"></a>自适应梯度法</h5><p>通过将learning rate除以S的平方根进行更新。S指的是历史和当前的梯度的平方的累加。</p>
<img src="../../../img/image-20200806112619507.png" alt="image-20200806112619507" style="zoom:25%;">

<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>靠近极小值收敛速度减慢</li>
<li>直线搜索会出现问题</li>
<li>之字形下降（可能跑的方向不对）</li>
</ul>
<h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><p>只要在当前做一阶的求导即可</p>
<h3 id="牛顿法-寻找导数为0的点"><a href="#牛顿法-寻找导数为0的点" class="headerlink" title="牛顿法 - 寻找导数为0的点"></a>牛顿法 - 寻找导数为0的点</h3><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/37588590" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37588590</a></p>
</blockquote>
<p>迭代点处一阶导数和二阶导数对目标函数进行近似</p>
<p>核心：</p>
<p>基于函数<strong>局部</strong>就是二次曲面，在泰勒展开只需要前两项（高斯曲面才有更高阶导）</p>
<p>在某点处用二次函数来近似目标函数，得到导数为0的方程，求解该方程，得到下一个迭代点。因为是用二次函数近似，因此可能会有误差，需要反复这样迭代，直到到达导数为0的点处</p>
<p>过程：</p>
<ul>
<li>二次函数近似</li>
<li>二次模型的极小值作为新的迭代点，并不断重复</li>
<li>直到求得满足精度的近似最小值</li>
</ul>
<h5 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h5><img src="../../../img/image-20200806133041450.png" alt="image-20200806133041450" style="zoom: 25%;">

<p>ps：$$p_k$$表示搜索方向。因为可能去最大和最小，所以符号不定（不同版本课本不一样）</p>
<p>没有步长：能直接算出局部最小值在什么地方，所以不需要步长</p>
<h4 id="牛顿法改进"><a href="#牛顿法改进" class="headerlink" title="牛顿法改进"></a>牛顿法改进</h4><h5 id="阻尼牛顿法"><a href="#阻尼牛顿法" class="headerlink" title="阻尼牛顿法"></a>阻尼牛顿法</h5><p>问题：</p>
<p>牛顿方向不区分上升/下降，可能跑反了</p>
<p>对局部做的二次函数近似不知道有多大</p>
<img src="../../../img/image-20200806140917265.png" alt="image-20200806140917265" style="zoom:25%;">

<p>注：<img src="../../../img/image-20200806140925013.png" alt="image-20200806140925013" style="zoom:25%;"> （也就是$$x_{k+1}$$的最小对应的$$\lambda$$）</p>
<h5 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h5><p>H阵和求逆复杂度太大</p>
<p>解决：利用一阶导数(J阵)拟合H阵，构造一个替代阵</p>
<p>替代阵产生方法有很多（通过一阶阵来生成替代阵）</p>
<h5 id="牛顿法优点"><a href="#牛顿法优点" class="headerlink" title="牛顿法优点"></a>牛顿法优点</h5><ul>
<li><p>二阶收敛，收敛速度快</p>
<p>牛顿法：用二次曲面拟合当前点的局部曲面</p>
<p>梯度下降：用一次平面拟合</p>
</li>
</ul>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>计算复杂度<ul>
<li>每一步都需要求解目标函数的H矩阵的逆矩阵，计算比较复杂</li>
</ul>
</li>
<li>目标函数可导性<ul>
<li>必须一阶二阶可导，H阵须正定</li>
</ul>
</li>
</ul>
<h3 id="最小二乘优化-要求fx形式"><a href="#最小二乘优化-要求fx形式" class="headerlink" title="最小二乘优化 - 要求fx形式"></a>最小二乘优化 - 要求fx形式</h3><blockquote>
<p><a href="https://blog.csdn.net/fangqingan_java/article/details/48948487" target="_blank" rel="noopener">https://blog.csdn.net/fangqingan_java/article/details/48948487</a></p>
</blockquote>
<p>具体函数形式的优化方法</p>
<p>目标函数： <img src="../../../img/image-20200806174123316.png" alt="image-20200806174123316" style="zoom:33%;"></p>
<p>r(x)是残差项，求的是r(x)的J阵</p>
<blockquote>
<img src="../../../img/image-20200806184207327.png" alt="image-20200806184207327" style="zoom:33%;">

<p><img src="../../../img/image-20200806184636092.png" alt="image-20200806184636092"></p>
</blockquote>
<h5 id="SVD分解法"><a href="#SVD分解法" class="headerlink" title="SVD分解法"></a>SVD分解法</h5><p>$$J = USV^T$$</p>
<img src="../../../img/image-20200806190813967.png" alt="image-20200806190813967" style="zoom:33%;">

<p>转为线性最小二乘，直接求解</p>
<h5 id="信赖域法"><a href="#信赖域法" class="headerlink" title="信赖域法"></a>信赖域法</h5><p>舍弃<img src="../../../img/image-20200806191038466.png" alt="image-20200806191038466" style="zoom: 50%;">中的第二项舍弃</p>
<p>这样在局部就是线性最小二乘（类似拟牛顿法）</p>
<img src="../../../img/image-20200806191511481.png" alt="image-20200806191511481" style="zoom:33%;">

<p>核心：能否在局部弄个线性最小二乘替代他，如果可以，就放大一点，如果不行，就放小一点</p>
<img src="../../../img/image-20200806193023680.png" alt="image-20200806193023680" style="zoom:25%;">

<p>解释：p就是要解的位移 </p>
<h5 id="Levenberg-Marquardt算法"><a href="#Levenberg-Marquardt算法" class="headerlink" title="Levenberg-Marquardt算法"></a>Levenberg-Marquardt算法</h5><p>是梯度下降和牛顿法折中</p>
<img src="../../../img/image-20200806195526385.png" alt="image-20200806195526385" style="zoom:33%;">

<p>v &gt;=0 不会改变原来的方向</p>
<p>取大：左边的$$J^TJ$$可以忽略，$$vI$$是个常数，就变成梯度下降。$$p = -J^Tr$$</p>
<p>取小：接近牛顿法 $$p = -(J^TJ)^TJ^Tr$$</p>
<h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><p>过目一下：</p>
<img src="../../../img/image-20200806200126273.png" alt="image-20200806200126273" style="zoom:33%;">

<p>注：接受$$x_{new}$$表示满足线性最小二乘</p>
<h3 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h3><p>介于梯度法和牛顿法之间</p>
<p>特点：只需要一阶导。比一阶导快，</p>
<img src="../../../img/image-20200807002015931.png" alt="image-20200807002015931" style="zoom:33%;">

<p>ps：梯度法找的是正交方向（Z字形绿色）共轭梯度法</p>
<p>共轭梯度法（红色）找的是共轭方向，一步到位</p>
<h5 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a>数学定义</h5><p>两个向量共轭，那存在对称半正定矩阵</p>
<p>$$p_i^T(Ap_j)=0$$ （A的目的是做个A的变换，相当于直接找到数据最主要成分，反映了局部的函数方向）</p>
<p>$$p_i^T$$和$$Ap_j$$正交，而$$p_i$$和$$p_j$$不正交</p>
<h5 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h5><img src="../../../img/image-20200814141930918.png" alt="image-20200814141930918" style="zoom:33%;">

<p>解析：求出$$x_{i+1}$$后，和当前梯度，以及前面点梯度都有关</p>
<h5 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h5><ul>
<li><p>步长的确定</p>
</li>
<li><p>假定局部线性</p>
</li>
<li><p>新共轭方向确定</p>
<ul>
<li><img src="../../../img/image-20200807003333007.png" alt="image-20200807003333007" style="zoom:25%;">

<p>假设$$p_k$$和前面k-1个$$p_i$$有关</p>
</li>
</ul>
</li>
</ul>
<img src="../../../img/image-20200807003441219.png" alt="image-20200807003441219" style="zoom: 33%;">



<h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>搜索前面的都是凸优化 找到的解就是最优解</p>
<p>方法：</p>
<p>非凸 转为 凸优化</p>
<blockquote>
<p>Machine Learning最核心的亮点</p>
</blockquote>
<ul>
<li><p>怎样找到目标函数，如何定义？</p>
</li>
<li><p>优化算法是什么？</p>
</li>
</ul>
<h4 id="TSP问题-NP完全问题"><a href="#TSP问题-NP完全问题" class="headerlink" title="TSP问题 - NP完全问题"></a>TSP问题 - NP完全问题</h4><ul>
<li><p>穷举法 BFS 保证最优解，但是复杂度O(N!)</p>
</li>
<li><p>贪婪搜索 不断重复选择最近且没有访问过的城市（每一步都在找全局最优解）复杂度O(NlogN)</p>
<p>无法保证最优解</p>
</li>
<li><p>爬山法 对当前解决方案的局部搜索，选择任一个选项来改善结果</p>
<ul>
<li>所有数值优化的迭代方法都是爬山法（比如梯度下降）</li>
</ul>
<h5 id="可能失效的原因"><a href="#可能失效的原因" class="headerlink" title="可能失效的原因"></a>可能失效的原因</h5><ul>
<li>陷入局部最小</li>
<li>各方同向的锅底（没有头绪）</li>
<li>函数存在很大的平面区域（和锅底类似，不过它连梯度也没）</li>
</ul>
</li>
</ul>
<h4 id="利用和探索-搜索的两种机制"><a href="#利用和探索-搜索的两种机制" class="headerlink" title="利用和探索 - 搜索的两种机制"></a>利用和探索 - 搜索的两种机制</h4><p>机器学习常常混合这两种机制</p>
<p>探索 - 穷举（尝试新的）</p>
<p>利用 - 爬山（利用旧的）</p>
<h4 id="模拟退火算法-启发式算法"><a href="#模拟退火算法-启发式算法" class="headerlink" title="模拟退火算法 - 启发式算法"></a>模拟退火算法 - 启发式算法</h4><p>利用的概率是玻尔兹曼分布</p>
<p>指数表示之前老虎机的收益</p>
<p><img src="../../../img/image-20200807091712218.png" alt="image-20200807091712218"></p>
<p>探索和利用的trade-off</p>
<h3 id="UCB算法"><a href="#UCB算法" class="headerlink" title="UCB算法"></a>UCB算法</h3><p>选择置信区间上界最大的臂</p>
<p>均值 + 玩了多少次</p>
<p>Ui反映了利用   后面反映了搜索（如果玩的次数少，说明要多玩）</p>
<h1 id="8-演化学习"><a href="#8-演化学习" class="headerlink" title="8 演化学习"></a>8 演化学习</h1><p>演化学习经常代表遗传算法。</p>
<p>遗传算法中有一系列的解，在自然界中形成竞争。（与以往参数模型算法的不同）</p>
<h3 id="遗传算法GA-解决非凸问题的随机优化方式（与前面优化方法的不同）"><a href="#遗传算法GA-解决非凸问题的随机优化方式（与前面优化方法的不同）" class="headerlink" title="遗传算法GA - 解决非凸问题的随机优化方式（与前面优化方法的不同）"></a>遗传算法GA - 解决非凸问题的随机优化方式（与前面优化方法的不同）</h3><p>对当前最好假设的重组来产生后续假设模型</p>
<p>不知道当前解对未来的结果是什么样的</p>
<h5 id="4个问题"><a href="#4个问题" class="headerlink" title="4个问题"></a>4个问题</h5><ul>
<li>表示解 - 二进制</li>
<li>解的好坏 - 适应度函数</li>
<li>怎么选择好的解</li>
<li>怎么通过好的解产生下一代</li>
</ul>
<h4 id="一般形式"><a href="#一般形式" class="headerlink" title="一般形式"></a>一般形式</h4><img src="../../../img/image-20200310164250819.png" alt="image-20200310164250819" style="zoom: 33%;">

<h5 id="假设模型的表示"><a href="#假设模型的表示" class="headerlink" title="假设模型的表示"></a>假设模型的表示</h5><img src="../../../img/image-20200807102532954.png" alt="image-20200807102532954" style="zoom:25%;">

<p>决策属性</p>
<p>1 - Yes 0 - No # - No/Yes</p>
<h5 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h5><p>背包问题中，用染色体表示拿哪种物品。适应函数是物品总价值。</p>
<h5 id="产生后代的方法"><a href="#产生后代的方法" class="headerlink" title="产生后代的方法"></a>产生后代的方法</h5><p>基于适应度函数</p>
<ul>
<li><p>锦标赛选择</p>
<p>放回抽样，然后选择最好的一个进入子代种群</p>
</li>
<li><p>截断选择</p>
<p>选择适应度最高的前k个</p>
<p>进行染色体复制，使子代种群达到父代种群规模</p>
</li>
<li><p>轮盘赌选择</p>
<p>基本思想是：各个个体被选中的概率与其适应度大小成正比</p>
</li>
</ul>
<h3 id="遗传算子"><a href="#遗传算子" class="headerlink" title="遗传算子"></a>遗传算子</h3><ol>
<li><p>对当前选择的染色体进行重组，产生后代</p>
<p>交叉 从交叉点开始 做片段交换</p>
<ul>
<li>单点交叉</li>
<li>两点交叉</li>
<li>均匀交叉</li>
</ul>
</li>
<li><p>变异 选择一个候选个体 随机选择一位 然后取反</p>
</li>
</ol>
<h5 id="后代演化方法"><a href="#后代演化方法" class="headerlink" title="后代演化方法"></a>后代演化方法</h5><ul>
<li><p>简易方案（也是自然方案）</p>
<p>子代代替父代</p>
</li>
<li><p>精英法</p>
<p>每一代保留上一代最优的染色体</p>
</li>
<li><p>锦标赛法</p>
<p>父母染色体和子代染色体竞争，胜者到下一种群</p>
</li>
<li><p>小生境法</p>
<img src="../../../img/image-20200807113631318.png" alt="image-20200807113631318" style="zoom:25%;">

</li>
</ul>
<h5 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h5><p>第0代：随机选取</p>
<p>第1代：取最好的4个，并且有交叉和变异</p>
<p>替换第0代适应度低的4个</p>
<p>……</p>
<p>如何表示解；如何评判解的好坏（适应度函数）；如何选择好的解释；通过好的解产生下一代。</p>
<p>优点：解很多，并行优势大</p>
<h3 id="模式定理"><a href="#模式定理" class="headerlink" title="模式定理"></a>模式定理</h3><p>short schemata with large fitness   –   increase presentation</p>
<p>0#10 - 0010/0110</p>
<h4 id="阶"><a href="#阶" class="headerlink" title="阶"></a>阶</h4><p>模式中确定位置的个数 O(#10##) = 2 （就是非#的数量）</p>
<h4 id="长度"><a href="#长度" class="headerlink" title="长度"></a>长度</h4><p>第一个确定的位置到最后一个确定位置之间的距离</p>
<p>O(##1#0) = 5-3=2</p>
<h4 id="模式理论"><a href="#模式理论" class="headerlink" title="模式理论"></a>模式理论</h4><p>m(s, t) 表示第t代种群中模式s的实例数量</p>
<p>通过m(s,t)推断t+1代中的实例数目</p>
<h5 id="模式进化"><a href="#模式进化" class="headerlink" title="模式进化"></a>模式进化</h5><p>轮盘赌选择<img src="../../../img/image-20200807120007752.png" alt="image-20200807120007752" style="zoom:25%;">（ps：最右边分母表示第t代染色体平均适应度）</p>
<img src="../../../img/image-20200807120429849.png" alt="image-20200807120429849" style="zoom:33%;">

<h5 id="模式定理-1"><a href="#模式定理-1" class="headerlink" title="模式定理"></a>模式定理</h5><ul>
<li>适应度越高模式影响力越大</li>
<li>有越多#影响力越大（染色体覆盖的实例越大，泛化性能强）</li>
<li>确定位彼此靠近的影响力越大</li>
</ul>
<p>为什么种群一代代适应度越高呢？  — 选择算子</p>
<p>交叉变异倾向于产生长度短，确定位少的模式</p>
<h3 id="遗传算法求解最短路径问题"><a href="#遗传算法求解最短路径问题" class="headerlink" title="遗传算法求解最短路径问题"></a>遗传算法求解最短路径问题</h3><ol>
<li><p>选择初始种群</p>
<p>从初始点开始，每次从该点的临接点中选择一个之前没有选择过的点，直到选择到终点</p>
</li>
<li><p>评估种群中每个个体的适应度</p>
<p>计算每个个体（路径）的路径总和</p>
</li>
<li><p>选择排名靠前的个体来reproduce</p>
<p>选择两个具有最短路径的个体（路径）</p>
</li>
<li><p>交叉和变异</p>
<p>随机从两个parent路径中选择公共点进行交叉，如图所示</p>
<img src="../../../img/image-20200810220603112.png" alt="image-20200810220603112" style="zoom:33%;">
</li>
<li><p>计算子类的适应度</p>
<p>（也可以直接用锦标赛法，上一代和下一代，一起选择前K个就行了）</p>
<p>如果子类适应度低于种群中最大的适应度，那么把子结点用最大适应度对应的结点替换</p>
<img src="../../../img/image-20200810224132387.png" alt="image-20200810224132387" style="zoom:33%;">
</li>
<li><p>终止条件</p>
<p>达到预先定义迭代次数停止（在网络拓扑中无法寻找全局最优解）</p>
</li>
</ol>
<h1 id="11-强化学习"><a href="#11-强化学习" class="headerlink" title="11 强化学习"></a>11 强化学习</h1><p>通过ML方法，实现认知的强化</p>
<h5 id="本质：奖惩与试错"><a href="#本质：奖惩与试错" class="headerlink" title="本质：奖惩与试错"></a>本质：奖惩与试错</h5><p>交互学习：通过交互产生的样本。什么机器决定了产生什么样的样本。</p>
<h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><ul>
<li>决策（状态→动作的映射）</li>
<li>最大化长期奖赏</li>
</ul>
<p>马尔可夫决策过程</p>
<blockquote>
<p>一阶马尔可夫性：未来的状态与历史状态无关</p>
</blockquote>
<p>S a delta R</p>
<h2 id="MDP模型"><a href="#MDP模型" class="headerlink" title="MDP模型"></a>MDP模型</h2><p>样本不是提前给的</p>
<p>学的不是分界平面，而是一种策略</p>
<h4 id="轨迹"><a href="#轨迹" class="headerlink" title="轨迹"></a>轨迹</h4><p>一次episode中，获得的经验或者轨迹。</p>
<h4 id="返回函数"><a href="#返回函数" class="headerlink" title="返回函数"></a>返回函数</h4><p>即时奖赏值的线性组合</p>
<ul>
<li>有限窗口 10步以内</li>
<li>无穷窗口 <ul>
<li>有折扣 最常用</li>
<li>无折扣</li>
</ul>
</li>
</ul>
<h4 id="动作选择"><a href="#动作选择" class="headerlink" title="动作选择"></a>动作选择</h4><p>一定会</p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>最重要：MDP状态转换和reward一致</p>
<p>调整Vpi 0 最优控制</p>
<h4 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h4><p>这个策略下，获得的值函数是什么</p>
<h4 id="控制"><a href="#控制" class="headerlink" title="控制"></a>控制</h4><p>某个点等于当前状态加上后续状态集合</p>
<h3 id="Mento-Carlo策略评价"><a href="#Mento-Carlo策略评价" class="headerlink" title="Mento Carlo策略评价"></a>Mento Carlo策略评价</h3><p>一直采用$$\pi$$策略，玩到底</p>
<p>是一种采样方法</p>
<p>如果采样是充分的，那么可以计算走每个分支的概率</p>
<p>遇到环路：</p>
<p>只让第一次遇到的进行平均</p>
<h4 id="时差学习"><a href="#时差学习" class="headerlink" title="时差学习"></a>时差学习</h4><p>样本利用是低效的</p>
<p>提高利用率的方法：时差学习TD 充分利用每一步的经验</p>
<p>代替从轨迹上获得的</p>
<h4 id="监督学习-vs-强化学习"><a href="#监督学习-vs-强化学习" class="headerlink" title="监督学习 vs 强化学习"></a>监督学习 vs 强化学习</h4><h4 id="离策略-在策略"><a href="#离策略-在策略" class="headerlink" title="离策略 在策略"></a>离策略 在策略</h4><p>基于Q的学习</p>
<h5 id="Q-学习算法"><a href="#Q-学习算法" class="headerlink" title="Q-学习算法"></a>Q-学习算法</h5><img src="../../../img/image-20200319161256969.png" alt="image-20200319161256969" style="zoom:50%;">

<p>离策略，与当前使用的策略没关系。</p>
<h5 id="SARSA算法"><a href="#SARSA算法" class="headerlink" title="SARSA算法"></a>SARSA算法</h5><img src="../../../img/image-20200319161314232.png" alt="image-20200319161314232" style="zoom:50%;">

<p>没有max，a’ 意味着当前使用的策略（pi指定的，即on-policy）去更新。</p>
<h3 id="N步TD预测"><a href="#N步TD预测" class="headerlink" title="N步TD预测"></a>N步TD预测</h3><p>揭示了强化学习重要特点：延迟回报</p>
<p>每一步都要更新原先的每一步（蒙特卡洛是更新链上的每一个）</p>
<p>构建一个估计，考虑到每一步（一步/两步更新）</p>
<blockquote>
<p>1.初始化V(s), e(s)=0</p>
<p>2.对每一个episode，重复</p>
<p>​    初始化s</p>
<p>​    对episode中的每一步</p>
<p>​        根据ε-贪心策略选择动作a</p>
<p>​        执行动作a，获得r和s′</p>
<p>​        ∆←r+γV(s′ )-V(s)</p>
<p>​        e(s)←e(s)+1</p>
<p>​        对于所有s</p>
<p>​           V(s)←V(s)+α∆e(s)</p>
<p>​           e(s)←γλe(s)</p>
<p>​        s←s^′</p>
<p>​    直到s为终止状态</p>
</blockquote>
<h3 id="关系强化学习"><a href="#关系强化学习" class="headerlink" title="关系强化学习"></a>关系强化学习</h3><h1 id="10-树学习"><a href="#10-树学习" class="headerlink" title="10 树学习"></a>10 树学习</h1><h3 id="符号-概念-学习和变型空间"><a href="#符号-概念-学习和变型空间" class="headerlink" title="符号(概念)学习和变型空间"></a>符号(概念)学习和变型空间</h3><p>哲学和智能是相通的</p>
<ul>
<li>演绎推理 P-&gt;Q, P真Q真【一般原则应用到具体事件】</li>
<li>反绎推理 P-&gt;Q，Q真P真（医学，不可靠）</li>
<li>归纳推理 已知前件为真，后件未必为真（训练集100%/假设正确，用到测试集上不一定为100%/假设正确）但不妨我使用这个假设【个别事件提取公共属性，西瓜是甜的，香瓜是甜的-&gt;所有瓜都是甜的】</li>
</ul>
<p><code>符号学习是一类归纳推理，尽管不可靠，但是有用</code></p>
<p>机器学习的中心问题：从特殊的训练样例中归纳出通用式函数（<strong>管中窥豹</strong>）</p>
<h4 id="概念学习"><a href="#概念学习" class="headerlink" title="概念学习"></a>概念学习</h4><p>给定样例集合，以及每个样例是否属于某个概念（是否能出去玩），自动推断出概念的一般定义（什么样的情况下能出去玩）。</p>
<blockquote>
<p><em>从某个布尔函数的输入、输出样例中推断出该<strong>布尔函数</strong>。（机器学习就是找函数-找布尔函数）</em></p>
</blockquote>
<p>符号学习是一类归纳推理</p>
<p>实例集合</p>
<h5 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h5><p>寻找一个假设h，使得对于实例集合X中的所有x，h(x)=c(x)</p>
<p>c(x)表示目标概念，定义在实例集上的布尔函数 c:Xà{0,1}。（就是能获得数据的ground truth 标签）</p>
<h5 id="实例空间和假设"><a href="#实例空间和假设" class="headerlink" title="实例空间和假设"></a>实例空间和假设</h5><?, ?, ?><p> 取什么都可以</p>
<p>&lt;$, $, $&gt; 取什么都不行</p>
<p>做假设的时候还要加上这两种</p>
<p>假设空间比样本空间大（维数灾难）</p>
<blockquote>
<p>假设：任一假设如果在足够大的训练样例集合中能很好的逼近目标概念函数，它也能在未见实例中很好的逼近目标概念</p>
<p>（也就是说没必要给出所有可能的样本集合）</p>
</blockquote>
<h5 id="搜索的概念学习"><a href="#搜索的概念学习" class="headerlink" title="搜索的概念学习"></a>搜索的概念学习</h5><p>搜索能最好拟合训练样本的假设</p>
<h4 id="Find-S-（笨方法）-寻找极大特殊假设"><a href="#Find-S-（笨方法）-寻找极大特殊假设" class="headerlink" title="Find-S （笨方法）- 寻找极大特殊假设"></a>Find-S （笨方法）- 寻找极大特殊假设</h4><img src="../../../img/image-20200808102728182.png" alt="image-20200808102728182" style="zoom:33%;">

<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><p>样本可能不充分，所以可能不是特殊的假设</p>
<p>如果出现多个极大假设，也没办法处理</p>
<h5 id="变型空间-version"><a href="#变型空间-version" class="headerlink" title="变型空间 - version"></a>变型空间 - version</h5><p>包含反例</p>
<p>包含所有满足样本一致性的集合（包括正例和反例）</p>
<h4 id="列表消除算法"><a href="#列表消除算法" class="headerlink" title="列表消除算法"></a>列表消除算法</h4><p>穷举法</p>
<img src="../../../img/image-20200808105219990.png" alt="image-20200808105219990" style="zoom:33%;">

<p>问题：列出所有的假设是不可能的，指数级复杂度</p>
<p>极大泛化：G集合 最一般假设</p>
<p>极大特化：S集合 最特殊假设 所有的H都和样本满足，但找不到还有一个H’，比H还特化</p>
<img src="../../../img/image-20200808105557434.png" alt="image-20200808105557434" style="zoom:25%;">

<p>上图的最上方就是极大特化，最下方就是极大泛化</p>
<h5 id="正例-负例的作用"><a href="#正例-负例的作用" class="headerlink" title="正例 负例的作用"></a>正例 负例的作用</h5><p>正例：将S集合泛化（逐步把概念的外沿<strong>扩大</strong>）</p>
<p>负例：将G集合特化（<strong>缩小</strong>概念）</p>
<h4 id="候选消除算法-爬山法"><a href="#候选消除算法-爬山法" class="headerlink" title="候选消除算法 - 爬山法"></a>候选消除算法 - 爬山法</h4><img src="../../../img/image-20200813203200973.png" alt="image-20200813203200973" style="zoom:33%;">

<p>初始化两个集合S（全为$），G（全为?）</p>
<p>对于正例：让S变得更一般</p>
<p>对于负例：修正G，排除掉负例中可能导致标签为负的属性（并且还要避免和S的冲突）</p>
<img src="../../../img/image-20200808144405813.png" alt="image-20200808144405813" style="zoom:33%;">

<p>注意：要保证G的泛化要在S的特化范围内（也就是不产生冲突）</p>
<h6 id="Find-S算法只找到了S，而爬山法找到了G，最终结果就在S和G之间"><a href="#Find-S算法只找到了S，而爬山法找到了G，最终结果就在S和G之间" class="headerlink" title="Find-S算法只找到了S，而爬山法找到了G，最终结果就在S和G之间"></a>Find-S算法只找到了S，而爬山法找到了G，最终结果就在S和G之间</h6><h3 id="归纳偏置-Inductive-bias-给定某种形式的预先假定"><a href="#归纳偏置-Inductive-bias-给定某种形式的预先假定" class="headerlink" title="归纳偏置 Inductive bias - 给定某种形式的预先假定"></a>归纳偏置 Inductive bias - 给定某种形式的预先假定</h3><blockquote>
<p><a href="https://blog.csdn.net/u011938325/article/details/75173140" target="_blank" rel="noopener">https://blog.csdn.net/u011938325/article/details/75173140</a></p>
</blockquote>
<p>进行概念学习的时候，定义的假设空间可能出问题</p>
<p>假设空间是合取式（属性值的合取，有偏，就是把并集直接当作？），真实空间是析取式（允许或的概念）</p>
<h5 id="无偏"><a href="#无偏" class="headerlink" title="无偏"></a>无偏</h5><p>允许析取/允许或运算</p>
<img src="../../../img/image-20200808150723015.png" alt="image-20200808150723015" style="zoom:33%;">

<p>问题：无法构造用于无偏学习的算法，因为在无偏学习中，没法进行泛化（正例就是正例，负例就是负例，什么也学不出来）</p>
<p>结论：归纳学习必须给定某种形式的预先假定（但学出来的模型是假定下的）</p>
<h5 id="三种归纳学习算法"><a href="#三种归纳学习算法" class="headerlink" title="三种归纳学习算法"></a>三种归纳学习算法</h5><ul>
<li>机械式学习（没有归纳偏置没法学）</li>
<li>候选消除 会学出一个变形空间</li>
<li>Find-S（比候选消除还有偏）eg：小女孩胆子小，没声明为正例的全是负例</li>
</ul>
<p>有偏性越强，学习器的归纳能力越强</p>
<p>如何学习有析取表示的假设空间？ - 决策树</p>
<h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h3><h5 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h5><p>特点：<strong>能学习析取表达式</strong></p>
<p>归纳偏置：优先选择较小的树</p>
<p>目标：输出深度最小的树</p>
<h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><img src="../../../img/image-20200808181928927.png" alt="image-20200808181928927" style="zoom:33%;">

<img src="../../../img/image-20200808181947255.png" alt="image-20200808181947255" style="zoom: 33%;">

<h5 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h5><img src="../../../img/image-20200808221658851.png" alt="image-20200808221658851" style="zoom:33%;">

<h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul>
<li>仅维持单一的当前假设（不同于变型空间候选消除算法 - 维持满足训练样例的所有假设）</li>
<li>局部最优 （不进行回溯） 改进是C4.5</li>
<li>基于统计<ul>
<li>对错误样例不敏感</li>
<li>不适于增量处理</li>
</ul>
</li>
</ul>
<h5 id="决策树中的归纳偏置"><a href="#决策树中的归纳偏置" class="headerlink" title="决策树中的归纳偏置"></a>决策树中的归纳偏置</h5><p>优先选择较短，信息增益更高的树（搜索策略决定了归纳偏置）</p>
<h3 id="其他树算法"><a href="#其他树算法" class="headerlink" title="其他树算法"></a>其他树算法</h3><img src="../../../img/image-20200808223732101.png" alt="image-20200808223732101" style="zoom:33%;">

<p>剪枝</p>
<p>完全生长 -&gt; 泛化性能弱</p>
<ul>
<li>前剪枝</li>
</ul>
<h1 id="11-集成学习"><a href="#11-集成学习" class="headerlink" title="11 集成学习"></a>11 集成学习</h1><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>一个预测模型的元方法，不是机器学习算法</p>
<p>多个学习器组合起来，结合模块输出</p>
<h5 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h5><p><u>如何分类？如何集成？</u></p>
<p>多个分类器集成起来，提高分类准确率</p>
<p>由训练数据构建基分类器，根据预测结果进行投票</p>
<h6 id="准确度和多样性之间要有权衡"><a href="#准确度和多样性之间要有权衡" class="headerlink" title="准确度和多样性之间要有权衡"></a>准确度和多样性之间要有权衡</h6><h5 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h5><ul>
<li>序列集成 基学习器之间的依赖关系<ul>
<li>目标：减小偏差</li>
</ul>
</li>
<li>并行集成 <ul>
<li>目标：减小方差</li>
</ul>
</li>
</ul>
<h5 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h5><ul>
<li><p>平均法（回归）</p>
</li>
<li><p>投票法（分类）</p>
</li>
<li><p>学习法</p>
<ul>
<li><blockquote>
<p>上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p>
<p>在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h5 id="多样性策略"><a href="#多样性策略" class="headerlink" title="多样性策略"></a>多样性策略</h5><ul>
<li>数据层面</li>
<li>属性层面</li>
<li>参数层面</li>
</ul>
<h3 id="Bagging-（Bootstrap-Aggregation）"><a href="#Bagging-（Bootstrap-Aggregation）" class="headerlink" title="Bagging （Bootstrap Aggregation）"></a>Bagging （Bootstrap Aggregation）</h3><p>原理</p>
<p><strong>有放回的采样</strong>，得到统计量分布和置信空间。 </p>
<p>与boosting的不同：弱分类器间没有依赖关系，可以并行生成，最后将预测结果继承</p>
<p>bias必须大于0</p>
<img src="../../../img/image-20200809101157697.png" alt="image-20200809101157697" style="zoom:33%;">



<h4 id="代表算法-随机森林"><a href="#代表算法-随机森林" class="headerlink" title="代表算法 - 随机森林"></a>代表算法 - 随机森林</h4><blockquote>
<p><a href="https://easyai.tech/ai-definition/random-forest/" target="_blank" rel="noopener">https://easyai.tech/ai-definition/random-forest/</a></p>
</blockquote>
<p>随机森林：很多决策树构成，不同决策树之间没有关联</p>
<p>分类任务：输入样本进入后，让森林中的每棵决策树进行判断和分类，最后统计哪种分类结果最多，就将其作为最终结果哦</p>
<h5 id="构建RF方法："><a href="#构建RF方法：" class="headerlink" title="构建RF方法："></a>构建RF方法：</h5><p>四步：</p>
<ol>
<li>假如有N个样本，则有放回的随机选择n个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的n个样本用来训练一个决策树，作为决策树根节点处的样本。</li>
<li>当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &lt;&lt; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。</li>
<li>决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>
<li>按照步骤1~3建立大量的决策树，这样就构成了随机森林了。</li>
</ol>
<img src="../../../img/image-20200809111436698.png" alt="image-20200809111436698" style="zoom:33%;">

<h5 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h5><p>降低分类器方差，改善<strong>泛化</strong></p>
<p>速度快（并行）</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><blockquote>
<p>Boosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。</p>
<p>先基于初始训练集训练一个基学习器，再根据基学习器的性能表<strong>现对训练样本分布进行调整</strong>，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本来训练下一个基学习器。直至基学习器数量达到指定数目，最后将所有基学习器的结果进行加权结合。</p>
</blockquote>
<p>强依赖关系，各子学习器<strong>依次</strong>训练（<strong>串行、迭代</strong>）</p>
<h5 id="PAC学习理论"><a href="#PAC学习理论" class="headerlink" title="PAC学习理论"></a>PAC学习理论</h5><p>能否从假设空间H中学到一个好的假设h</p>
<ul>
<li>近似正确 泛化误差足够小</li>
<li>可能正确 多大概率达到近似正确</li>
</ul>
<img src="../../../img/image-20200809175448660.png" alt="image-20200809175448660" style="zoom:25%;">



<p><img src="../../../img/image-20200809173121047.png" alt="image-20200809173121047"></p>
<p>在多项式时间学习到一个近似正确的结果 - 强学习器</p>
<p>框架</p>
<p>bias不一定大于0 因为可能弱模型</p>
<p>转化：</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><blockquote>
<p>GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p>
</blockquote>
<p>决策树包括分类树和回归树（可以做数学运算）</p>
<h5 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h5><p>初始化弱分类器</p>
<p>循环</p>
<p>​    对每个样本计算负梯度</p>
<p>​    根据残差构建新的样本集合</p>
<p>​    构建CART树</p>
<p>​    计算叶子区域最佳拟合值</p>
<p>​    更新得到强学习器</p>
<p>得到最终强学习器</p>
<h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><blockquote>
<p> 推导过程：<a href="https://www.cnblogs.com/pinard/p/6133937.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6133937.html</a></p>
</blockquote>
<h5 id="PAC学习理论-1"><a href="#PAC学习理论-1" class="headerlink" title="PAC学习理论"></a>PAC学习理论</h5><p>强弱学习器等价</p>
<p>可以通过boosting将弱分类器转换为</p>
<p>训练过程和boosting思想相同，区别是Adaboost的损失函数变成了指数损失函数</p>
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>将几个学习器的预测结果作为新的训练集（新的特征），来学习一个新的学习器</p>
<img src="../../../img/image-20200809205725222.png" alt="image-20200809205725222" style="zoom:33%;">

<p>优势：降低泛化误差，同时降低方差和偏差</p>
<p><img src="../../../img/image-20200809210817053.png" alt="image-20200809210817053"></p>
<h1 id="12-无监督学习"><a href="#12-无监督学习" class="headerlink" title="12 无监督学习"></a>12 无监督学习</h1><blockquote>
<p>特征选取和距离度量很重要</p>
<p>核心：簇内相似+簇间不相似</p>
</blockquote>
<p>分析没有标签的数据</p>
<p>从无标记的数据学特征表示/预测</p>
<p>【<strong>物以类聚，人以群分</strong>】</p>
<ul>
<li><p>聚类的好坏不存在绝对标准(depends on opinion of the user)</p>
</li>
<li><p>总是能通过定义新的标准，来得到新的聚类方法（分类标准）</p>
</li>
</ul>
<h3 id="聚类相关概念"><a href="#聚类相关概念" class="headerlink" title="聚类相关概念"></a>聚类相关概念</h3><p>簇：簇内对象相似，不同簇间不相似</p>
<p>聚类算法：给出<strong>相似性评价标准</strong>，把数据集来划分</p>
<p>依据：样本-&gt;特征向量-&gt;特征空间的点，通过衡量点之间的距离来进行聚类</p>
<p>好的聚类算法：</p>
<ul>
<li><strong>簇内高相似</strong></li>
<li><strong>簇间低相似</strong></li>
</ul>
<p>聚类的目的：</p>
<p>对数据一无所知，目的是对数据做划分和可视化</p>
<ul>
<li>潜在的自然分组结构</li>
<li>感兴趣的关系</li>
</ul>
<h5 id="影响聚类的因素："><a href="#影响聚类的因素：" class="headerlink" title="影响聚类的因素："></a>影响聚类的因素：</h5><blockquote>
<ul>
<li><strong>特征</strong>的选取和设计   特征对聚类很重要，选择什么特征，决定了在特征空间的分布</li>
<li><strong>距离函数</strong></li>
</ul>
</blockquote>
<p>聚类分析的有效性：</p>
<p>与<strong>数据特征向量的分布形式</strong>有很大关系（特征选的好，数据分布好，不同种类的样本聚在一块的话不好聚类）</p>
<p>对具体数据做聚类分析的关键是 <strong><u>特征的选取</u></strong></p>
<h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><p>度量同一类样本的<strong>相似性</strong>和不同类样本的<strong>差异性</strong></p>
<h4 id="常用度量函数"><a href="#常用度量函数" class="headerlink" title="常用度量函数"></a>常用度量函数</h4><img src="../../../img/image-20200810230142564.png" alt="image-20200810230142564" style="zoom:33%;">

<p>n维空间中两个点x1(x11,x12,…,x1n)与 x2(x21,x22,…,x2n)</p>
<p><strong>闵可夫斯基距离：</strong></p>
<img src="../../../img/image-20200401183852263.png" alt="image-20200401183852263" style="zoom:25%;">

<p>也可以表示为 <img src="../../../img/image-20200401183956670.png" alt="image-20200401183956670" style="zoom:33%;"></p>
<ul>
<li><p>曼哈顿 L1范数 用于L1正则化（为了矩阵可逆） Lasso回归</p>
</li>
<li><p>欧式 L2范数 用于L2正则化 Ridge回归</p>
<img src="../../../img/image-20200401183509680.png" alt="image-20200401183509680" style="zoom:25%;">

</li>
</ul>
<blockquote>
<p><a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">机器学习中正则化项L1和L2的直观理解</a></p>
</blockquote>
<ul>
<li><p>切比雪夫 L∞范数</p>
</li>
<li><p>余弦 点积/模的乘积  衡量的是相似性</p>
<p>优点：根据向量方向判断相似度，不受各个维度直接数值的影响</p>
</li>
<li><p>马氏距离   <strong>M矩阵 协方差矩阵的逆矩阵</strong></p>
<p>对欧式距离的修正，修正了<strong>欧式距离中各个维度尺度不一致且相关的问题</strong>。</p>
<p><a href="https://zhuanlan.zhihu.com/p/37609917" target="_blank" rel="noopener">协方差矩阵</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/46626607" target="_blank" rel="noopener">马氏距离通俗解释</a></p>
<ul>
<li>度量学习 通过优化算法 从数据中学M矩阵 来度量两个特征向量的距离</li>
</ul>
</li>
</ul>
<p>  单个点：</p>
  <img src="../../../img/image-20200407122557873.png" alt="image-20200407122557873" style="zoom:50%;">

<p>  两个点：</p>
  <img src="../../../img/image-20200407122623431.png" alt="image-20200407122623431" style="zoom:50%;">



<p><strong>度量空间</strong>是一个有序对<code>(X,d)</code>X是集合，d是度量函数</p>
<p>d把X中的每一对点映射为非负实数，并且满足四条公理</p>
<ul>
<li>非负</li>
<li>唯一</li>
<li>对称    不满足这条，叫伪度量（eg：KL散度）</li>
<li>三角不等式</li>
</ul>
<h3 id="聚类准则"><a href="#聚类准则" class="headerlink" title="聚类准则"></a>聚类准则</h3><p>类：定义有很多种，划分具有人为规定性</p>
<p>类定义：</p>
<p><code>d(xi, xj) &lt;= h</code>h为阈值</p>
<p>聚类需要 <strong>距离度量</strong> 和 <strong>聚类准则</strong>。(意思就是把距离为多少的归为一类)</p>
<p>需要一个能对聚类过程/聚类结果的优劣进行评估的准则函数。如果聚类准则函数选择的好，质量会更高。</p>
<ul>
<li>试探方法   定义阈值，按最近邻来指定属于某个聚类样别<ul>
<li>欧式距离-距离做阈值  余弦距离-相似度做阈值</li>
</ul>
</li>
<li>函数方法   聚类准则是反映类别间相似性或者分离性的函数。聚类分析转化为<strong>求准则函数极值</strong><ul>
<li>找样本集和类别之间的函数</li>
</ul>
</li>
</ul>
<img src="../../../img/image-20200409152821818.png" alt="image-20200409152821818" style="zoom: 33%;">



<h3 id="聚类方法"><a href="#聚类方法" class="headerlink" title="聚类方法"></a>聚类方法</h3><ul>
<li><p>基于试探的聚类搜索方法</p>
<ul>
<li><p>按最近邻规则的简单试探法</p>
<p>高维样本很难获得准确的先验知识，只能选不同的阈值和起点来试探</p>
<h5 id="影响因素"><a href="#影响因素" class="headerlink" title="影响因素"></a>影响因素</h5><ul>
<li>第一个中心点位置</li>
<li>待分类样本的排列次序</li>
<li>阈值T</li>
<li>样本的几何分布</li>
</ul>
<h5 id="算法过程-1"><a href="#算法过程-1" class="headerlink" title="算法过程"></a>算法过程</h5><p>给定：样本集、阈值T</p>
<p>基本思想：大于阈值就成为新中心，否则成为小于阈值且距离最小的那个类</p>
<img src="../../../img/image-20200409154118285.png" alt="image-20200409154118285" style="zoom: 25%;">





</li>
</ul>
</li>
</ul>
<ul>
<li><p>最大最小距离算法</p>
<p>随机选一个聚类中心z1，然后选距离z1欧式距离最大的点作为距离中心</p>
<h5 id="过程："><a href="#过程：" class="headerlink" title="过程："></a>过程：</h5><p>核心：找出各个样本离自己最近的中心点，然后选出距离最大的，如果超过最初两个中心距离的一定比例，那么可以作为新的中心，否则找中心结束。</p>
<img src="../../../img/image-20200409202754823.png" alt="image-20200409202754823" style="zoom: 33%;">

</li>
</ul>
<ul>
<li><p>系统聚类法</p>
<ul>
<li><p>按距离准则逐步分类（类别由多到少，合并点）</p>
<p>核心：形成nxn的距离矩阵，找出其中的最小元素aij，那么把i和j类进行合并；</p>
<p>​                更新距离矩阵</p>
<p>​                达到所需的聚类数目，或者矩阵中的最小分量超过给定阈值D停止。</p>
<ul>
<li>距离准则函数<ul>
<li>最短距离 两个集合所有距离的最小值</li>
<li>最长距离  所有距离的最大值</li>
<li>类平均距离 所有距离的平均值</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>动态聚类法（KMeans，ISODATA）</p>
<ul>
<li><p>KMeans</p>
<p>结束条件：重新计算后聚类不发生样本所属聚类改变的情况</p>
<p>影响因素</p>
<ul>
<li>K</li>
<li>初始选的K个样本点</li>
<li>模式样本的几何性质（天然，影响最重要）</li>
</ul>
<p>如果数据样本可以形成若干个相距较远的孤立区域分布最好</p>
<p>适合分类数目已知的情况（K）</p>
<h5 id="算法流程-3"><a href="#算法流程-3" class="headerlink" title="算法流程"></a>算法流程</h5><img src="../../../img/image-20200813211656786.png" alt="image-20200813211656786" style="zoom:33%;">
</li>
</ul>
</li>
<li><p>KMeans++</p>
<ul>
<li><p>K个初始聚类中心分的越开越好</p>
<ul>
<li>均匀分布的采样，选择下一个聚类中心</li>
</ul>
<p><a href="https://blog.csdn.net/The_lastest/article/details/78288955" target="_blank" rel="noopener">KMeans++例子</a></p>
<img src="../../../img/image-20200409210518936.png" alt="image-20200409210518936" style="zoom:33%;">
</li>
<li><p>ISODATA算法（迭代自组织数据分析算法）</p>
<ul>
<li>分裂和合并</li>
</ul>
<img src="../../../img/image-20200409221713498.png" alt="image-20200409221713498" style="zoom: 33%;">

<img src="../../../img/image-20200409221924382.png" alt="image-20200409221924382" style="zoom: 33%;">

<img src="../../../img/image-20200409221947890.png" alt="image-20200409221947890" style="zoom:33%;">

<p>dmin(聚类中心的最小距离)和Sigma(样本的离散程度)作为是否进行合并和分裂的标准</p>
</li>
</ul>
</li>
</ul>
<p>  比较</p>
<ul>
<li>K-means适合类别数目已知的聚类</li>
<li>算法角度：相似，中心都是通过样本均值的迭代来决定<ul>
<li>ISODATA加入了试探步骤，利用中间经验进行更好的分类</li>
</ul>
</li>
<li>ISODATA需要指定比较多的参数，并且很难准确指定某个值，因此不太受欢迎</li>
</ul>
<h3 id="聚类评价"><a href="#聚类评价" class="headerlink" title="聚类评价"></a>聚类评价</h3><ul>
<li>聚类中心之间的距离<ul>
<li>距离过大 需要分裂</li>
</ul>
</li>
<li>聚类域中的样本数目<ul>
<li>数目少，并且聚类中心距离远，可能是噪声</li>
</ul>
</li>
<li>域内的样本方差<ul>
<li>方差太大可能不属于这一类</li>
</ul>
</li>
</ul>
<h4 id="常用评价指标"><a href="#常用评价指标" class="headerlink" title="常用评价指标"></a>常用评价指标</h4><h5 id="标签未知"><a href="#标签未知" class="headerlink" title="标签未知"></a>标签未知</h5><ul>
<li><p>Compactness 紧密度CP（越小越紧凑）</p>
</li>
<li><p>Separation 间隔度SP （越大越分散）</p>
</li>
<li><p>DBI  戴维森堡丁指数<strong>/</strong>分类适确性指标 使用了CP和SP  越小越好（缺点：使用欧式距离，对环状分布聚类评价很差）</p>
</li>
<li><p>DVI 邓恩系数 越大越好（缺点：对离散点的聚类测评很高、对环状分布测评效果差）</p>
</li>
</ul>
<h5 id="标签已知（聚类结果和真实标签的差异）"><a href="#标签已知（聚类结果和真实标签的差异）" class="headerlink" title="标签已知（聚类结果和真实标签的差异）"></a>标签已知（聚类结果和真实标签的差异）</h5><p>聚类准确率</p>
<p>兰德指数</p>
<p>调整兰德指数</p>
<p>互信息</p>
<p>归一化互信息</p>
<h5 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h5><p> 二维高斯分布展示</p>
<p>图像分割（每个像素做聚类）</p>
<h2 id="概率与学习"><a href="#概率与学习" class="headerlink" title="概率与学习"></a>概率与学习</h2><h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><p>凸函数：曲线在直线下方  （表示函数的斜率一直在增加） eg: e^x   有最小值</p>
<p>凹函数：f(x)为凸函数，那么-f(x)为凹函数    eg: logx is concave in R++（最大化对数函数时，就可以求到最大值）</p>
<p>判定：</p>
<ul>
<li>f’’(x)&gt;=0 -&gt; 凸函数</li>
<li>Hessian矩阵是半正定的 -&gt; 凸函数（正定矩阵的特征值&gt;0)</li>
</ul>
<p>通常情况下，损失函数建立为凸函数比较好</p>
<p>最小化凸函数时，有最小值，能求出最优解（凸优化）</p>
<h5 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h5><h5 id="琴生不等式"><a href="#琴生不等式" class="headerlink" title="琴生不等式"></a>琴生不等式</h5><p>X是随机变量，f(X)是凸函数,  E(f(X))  &gt;= f(E(X))</p>
<p>X是随机变量，f(X)是凹函数,  E(f(X))  &lt;= f(E(X))</p>
<h5 id="高斯分布-正态分布"><a href="#高斯分布-正态分布" class="headerlink" title="高斯分布/正态分布"></a>高斯分布/正态分布</h5><p>应用最广泛的概率分布</p>
<ul>
<li><p>单变量高斯分布</p>
<p>方差越大，图像越矮胖</p>
<p>pdf：</p>
</li>
<li><p>多变量高斯分布</p>
<p>均值为miu，方差为sigma（是一个协方差矩阵）</p>
<blockquote>
<p>变量如果是标量，符号是小写不加粗</p>
<p>变量如果是向量，符号是小写且加粗</p>
</blockquote>
<p>pdf：sigma往往是半正定矩阵</p>
<p>含有马氏距离</p>
</li>
</ul>
<h3 id="高斯混合模型-Gaussian-Mixture-Model"><a href="#高斯混合模型-Gaussian-Mixture-Model" class="headerlink" title="高斯混合模型 Gaussian Mixture Model"></a>高斯混合模型 Gaussian Mixture Model</h3><p>两个高斯分布的加权</p>
<p>pdf：</p>
<p>需要乘ai（表示属于哪个高斯分布的概率），并且权重和为1</p>
<p>不知道有多少个高斯分布组成（可以根据类别数假设有多少个高斯分布）</p>
<p>需要求解三个参数</p>
<h3 id="最大似然估计-Maximum-likelihood-estimation-MLE"><a href="#最大似然估计-Maximum-likelihood-estimation-MLE" class="headerlink" title="最大似然估计 Maximum likelihood estimation MLE"></a>最大似然估计 Maximum likelihood estimation MLE</h3><h5 id="算法过程-2"><a href="#算法过程-2" class="headerlink" title="算法过程"></a>算法过程</h5><p>thelta：需要观测的参数</p>
<p>X：观测数据</p>
<p>i.i.d 独立同分布</p>
<p>乘积很小，因此取ln</p>
<h3 id="期望最大化算法-Expectiation-Maximization"><a href="#期望最大化算法-Expectiation-Maximization" class="headerlink" title="期望最大化算法 Expectiation-Maximization"></a>期望最大化算法 Expectiation-Maximization</h3><p>求解关于高斯函数的参数估计</p>
<p>迭代的方法</p>
<p>适合包含无法观测的隐变量的模型</p>
<h5 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h5><p>EM是一种迭代的方法，利用最大似然估计MLE对统计模型中的参数进行估计，特别是针对包含无法观测隐变量的模型</p>
<p>固定第一个变量，MLE求第二个；固定第二个，再使用MLE估测第一个；依次迭代直到收敛到局部最优解</p>
<h4 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h4><p>利用可观测数据$$\chi$$，和参数估计$$\theta^{(t)}$$，t表示第t次迭代，估计更好的隐藏变量Z</p>
<h4 id="M-step"><a href="#M-step" class="headerlink" title="M-step"></a>M-step</h4><p>利用可观测数据$$\chi$$和当前估计的隐藏变量Z，估计更好的参数$$\theta^{t+1}$$</p>
<h4 id="Repeat"><a href="#Repeat" class="headerlink" title="Repeat"></a>Repeat</h4><p>重复两个步骤，直到收敛</p>
<h4 id="优化分析"><a href="#优化分析" class="headerlink" title="优化分析"></a>优化分析</h4><p>为什么能求解模型参数呢？</p>
<p>先做了假设，对Q是什么不清楚</p>
<h4 id="EM-vs-K-means"><a href="#EM-vs-K-means" class="headerlink" title="EM vs K-means"></a>EM vs K-means</h4><p>高斯混合模型是一个软的K-means</p>
<p>K-means是<strong>直接指定</strong>类别，而高斯混合模型求的是Xi属于哪个高斯分布（类）的<strong>概率</strong></p>
<h4 id="K-means背后的EM思想"><a href="#K-means背后的EM思想" class="headerlink" title="K-means背后的EM思想"></a>K-means背后的EM思想</h4><p>聚类准则思想：</p>
<p>mj是类的均值，然后求解每个点到聚类中心距离的和</p>
<img src="../../../img/image-20200811103551391.png" alt="image-20200811103551391" style="zoom:33%;">







<h2 id="概率与学习（续）"><a href="#概率与学习（续）" class="headerlink" title="概率与学习（续）"></a>概率与学习（续）</h2><p>近朱者赤 近墨者黑</p>
<h3 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h3><p>训练集 x - 特征向量 y - 标签（ground truth）</p>
<p>分类 vs 回归：输出离散值（标签属于离散数值集合）  回归输出连续值（结果属于实数集）</p>
<p>计算张量之间的距离（矩阵之间距离，矩阵和向量之间距离，三维tensor）</p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>核心：分配给距离最近的N个邻居中占最大比例的类别</p>
<h5 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h5><p><img src="../../../img/image-20200811105117640.png" alt="image-20200811105117640"></p>
<h5 id="k的取值的影响"><a href="#k的取值的影响" class="headerlink" title="k的取值的影响"></a>k的取值的影响</h5><ul>
<li>奇数，避免平局</li>
<li>取不同值，分类结果不同（1，3，5，7，9，在具体任务中采用交叉验证选择具体的k）</li>
<li>k较小的时候，对噪声敏感，容易过拟合</li>
<li>k较大的时候，噪声不敏感，容易欠拟合（可选的特别多，每个样本都可能是我的近邻）</li>
</ul>
<h3 id="最近邻分类器-1-NN"><a href="#最近邻分类器-1-NN" class="headerlink" title="最近邻分类器 1-NN"></a>最近邻分类器 1-NN</h3><p>选择最近的训练样本</p>
<h4 id="泛化错误率"><a href="#泛化错误率" class="headerlink" title="泛化错误率"></a>泛化错误率</h4><p>泛化错误率≤2倍的贝叶斯最优分类器错误率</p>
<h3 id="K-近邻回归"><a href="#K-近邻回归" class="headerlink" title="K-近邻回归"></a>K-近邻回归</h3><p>选择k个最近的训练样本，将距离值的倒数作为权重，然后将k个近邻的标签值加权平均，作为回归结果。</p>
<img src="../../../img/image-20200811105522829.png" alt="image-20200811105522829" style="zoom:33%;">

<h4 id="近邻平滑"><a href="#近邻平滑" class="headerlink" title="近邻平滑"></a>近邻平滑</h4><ul>
<li>核平滑法<ul>
<li>二次核</li>
<li>次方核</li>
<li>高斯核</li>
</ul>
</li>
</ul>
<h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><h4 id="懒惰学习"><a href="#懒惰学习" class="headerlink" title="懒惰学习"></a>懒惰学习</h4><p>K-NN是典型的懒惰学习（训练阶段仅仅是把样本保存起来，训练时间开销为0，收集到测试样本再进行处理）</p>
<p>SVM、CNN是急切学习（训练阶段就进行处理，尝试在训练期间构造一个函数）</p>
<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul>
<h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>计算复杂度高（扫描整个训练集，复杂度高；对于急切学习，训练好后花时间，测试阶段非常快）</li>
<li>空间复杂度高（训练集需要存储起来）</li>
</ul>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><p>欧式距离，时间复杂度O(d)（因为要做d次计算，也就是d次向量相减）</p>
<p>ps：内积也是O(d)</p>
<p>训练阶段：0</p>
<p>测试阶段：O(nd+nlogk) nd是n个点的欧式距离，logk是排序</p>
<blockquote>
<p>思考：</p>
<p>n个数字选择k个最小的，时间复杂度？</p>
<p>K-NN空间复杂度？</p>
</blockquote>
<h3 id="降低近邻计算"><a href="#降低近邻计算" class="headerlink" title="降低近邻计算"></a>降低近邻计算</h3><ul>
<li><p>特征维度 2-5 维诺图</p>
<p>根据一组给定的目标，将一个平面划分成靠近每一个目标的多个区块</p>
<p>真实应用中不太实用</p>
</li>
<li><p>6-30 KD-Tree（K表示特征维度）</p>
<p><a href="https://zhuanlan.zhihu.com/p/23966698" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/23966698</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/45346117" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45346117</a></p>
<p>训练阶段构造KD树，测试阶段对树做检索</p>
<p>不断用垂直于坐标轴的超平面对K维空间切分，构成一系列的K维度超矩形区域。（每个节点对应一个K维超矩形区域）</p>
<h5 id="构造流程"><a href="#构造流程" class="headerlink" title="构造流程"></a>构造流程</h5><img src="../../../img/image-20200814101133933.png" alt="image-20200814101133933" style="zoom:33%;">



</li>
</ul>
<h5 id="KD-Tree-搜索"><a href="#KD-Tree-搜索" class="headerlink" title="KD-Tree 搜索"></a>KD-Tree 搜索</h5>  <img src="../../../img/image-20200814104111526.png" alt="image-20200814104111526" style="zoom:33%;">

<p>  以欧式距离为半径的圆与超平面不相交意味着没有更优解</p>
<h5 id="时间复杂度-1"><a href="#时间复杂度-1" class="headerlink" title="时间复杂度"></a>时间复杂度</h5><ul>
<li>O(nlog^2n) 快排、堆排序、归并排序</li>
<li>O(nlogn)  用median of meddians算法来寻找中位数</li>
<li>寻找最近邻：O(logn)</li>
</ul>
<ul>
<li><p>高维特征</p>
<ul>
<li><p>降维 PCA</p>
<p>原始高维属性空间 -&gt; 低维子空间</p>
</li>
<li><p>近似最近邻</p>
<p>不再只局限于返回最可能的数据项（不是最精确），只要找到近似的近邻就可以</p>
</li>
<li><p>哈希</p>
<p>把任意长度的输入映射为固定长度的输出</p>
<p>（把维度降下来，然后用0/1值来表示特征）</p>
</li>
</ul>
</li>
</ul>
<h3 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h3><p>k-NN缺点：不是建立在任何概率框架上</p>
<p>无法得到关于类别的后验概率，无法概率化推断近邻个数、度量参数。</p>
<p>小样本情况下，计算复杂度高的缺点被解决，在深度学习时代，仍然有较好的前景</p>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><h3 id="回顾-1"><a href="#回顾-1" class="headerlink" title="回顾"></a>回顾</h3><p>机器学习的目的是得到映射 X-&gt;Y</p>
<p>似然。p(x|y=i)</p>
<p>后验   p(y=i|x)</p>
<h5 id="概率框架角度"><a href="#概率框架角度" class="headerlink" title="概率框架角度"></a>概率框架角度</h5><ul>
<li><p>生成式模型</p>
<p>有先验概率和似然，用贝叶斯定理求后验概率</p>
</li>
<li><p>判别式模型</p>
<p>决策边界</p>
</li>
</ul>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>选择最好的决线性超平面</p>
<h3 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h3><h4 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h4><p>一个点的间隔margin式它到分界超平面的垂直距离</p>
<p>目标：最大化所有样本的最小边际margin</p>
<p>具有最小边际的点叫SV</p>
<p>只关心超平面的方向</p>
<h4 id="推导。。。"><a href="#推导。。。" class="headerlink" title="推导。。。"></a>推导。。。</h4><p>TODO</p>
<h5 id="soft-margin"><a href="#soft-margin" class="headerlink" title="soft margin"></a>soft margin</h5><p>允许少数点margin比1小</p>
<p>yif(xi) &gt;= 1(感知机大于0就行，但是这里要最大化最小间隔，所以要求高一些)   -&gt;  yif(xi) &gt;= 1-epsilon</p>
<h5 id="惩罚机制："><a href="#惩罚机制：" class="headerlink" title="惩罚机制："></a>惩罚机制：</h5><p>把epsilon当成函数来优化</p>
<p>把损失加入最小函数来最小化</p>
<p>非支持向量，拉格朗日乘子ai 等于0</p>
<h3 id="非线性SVM"><a href="#非线性SVM" class="headerlink" title="非线性SVM"></a>非线性SVM</h3><p>映射到高维空间，使得这个样本在特征空间内线性可分</p>
<p>通过内积 线性和非线性联系在一起</p>
<h5 id="kernal-trick"><a href="#kernal-trick" class="headerlink" title="kernal trick"></a>kernal trick</h5><p>将非线性函数表示为特征空间的内积</p>
<h5 id="Mercer’s-condition-充分必要"><a href="#Mercer’s-condition-充分必要" class="headerlink" title="Mercer’s condition 充分必要"></a>Mercer’s condition 充分必要</h5><h4 id="Kernel-SVM"><a href="#Kernel-SVM" class="headerlink" title="Kernel SVM"></a>Kernel SVM</h4><h5 id="非线性核"><a href="#非线性核" class="headerlink" title="非线性核"></a>非线性核</h5><h5 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h5><p>也需要通过交叉验证来调参数</p>
<h3 id="多类SVM"><a href="#多类SVM" class="headerlink" title="多类SVM"></a>多类SVM</h3><h4 id="多类"><a href="#多类" class="headerlink" title="多类"></a>多类</h4><p>转为2分类问题</p>
<p>构造Cn2个分类器</p>
<p>使用sigmod函数</p>
<p>投票，选择出现最多次数的类</p>
<h4 id="1对多"><a href="#1对多" class="headerlink" title="1对多"></a>1对多</h4><p>C个分类器</p>
<p>不用sigmod，直接输出作为信息</p>
<p>#### </p>
<p>以上这两种只是在训练和测试的时候改了形式，也有直接解决多类SVM的方法。</p>
</section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#机器学习" >
    <span class="tag-code">机器学习</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2019/10/23/C-数组总结/">
        <span class="nav-arrow">← </span>
        
          C++ 数组 &amp; 指针
        
      </a>
    
    
      <a class="nav-right" href="/2020/10/11/P、NP、NPC、NP-Hard问题的区别/">
        
          P、NP、NPC、NP-Hard问题的区别
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    <!---->
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
    <!-- 二维码 END -->
    
      <!-- No Comment -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
    <aside class="catalog-container">
  <div class="toc-main">
    <strong class="toc-title">Catalog</strong>
    
      <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#1-机器学习概论"><span class="toc-nav-text">1. 机器学习概论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#经典机器学习定义："><span class="toc-nav-text">经典机器学习定义：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#现代统计机器学习定义："><span class="toc-nav-text">现代统计机器学习定义：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#学习系统："><span class="toc-nav-text">学习系统：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#模型和学习层面"><span class="toc-nav-text">模型和学习层面</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#模型："><span class="toc-nav-text">模型：</span></a></li><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#学习层面："><span class="toc-nav-text">学习层面：</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#学习过程"><span class="toc-nav-text">学习过程</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#系统建模和模型选择"><span class="toc-nav-text">系统建模和模型选择</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#维数灾难"><span class="toc-nav-text">维数灾难</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#数据集划分方法"><span class="toc-nav-text">数据集划分方法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#建模相关："><span class="toc-nav-text">建模相关：</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#模型选择"><span class="toc-nav-text">模型选择</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#过拟合"><span class="toc-nav-text">过拟合</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#共性问题"><span class="toc-nav-text">共性问题</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#评价指标"><span class="toc-nav-text">评价指标</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#评价指标很重要，但指标又和数据有关系，相互影响"><span class="toc-nav-text">评价指标很重要，但指标又和数据有关系，相互影响</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#精度矩阵"><span class="toc-nav-text">精度矩阵</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#ROC曲线"><span class="toc-nav-text">ROC曲线</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#不平衡数据集"><span class="toc-nav-text">不平衡数据集</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#MCC-马修斯相关系数（衡量二分类）"><span class="toc-nav-text">MCC 马修斯相关系数（衡量二分类）</span></a></li><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#对数损失（可以来衡量多分类）"><span class="toc-nav-text">对数损失（可以来衡量多分类）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#测量精度"><span class="toc-nav-text">测量精度</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#模型选择-选择一个具有良好泛化性能的模型"><span class="toc-nav-text">模型选择 - 选择一个具有良好泛化性能的模型</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#四类方法："><span class="toc-nav-text">四类方法：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#吉洪诺夫正则化（Tikhonov）"><span class="toc-nav-text">吉洪诺夫正则化（Tikhonov）</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#先验的重要性"><span class="toc-nav-text">先验的重要性</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#泛化-数据-知识（与领域-任务无关的叫源知识）"><span class="toc-nav-text">泛化 = 数据 + 知识（与领域/任务无关的叫源知识）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#丑小鸭定理-特征表示"><span class="toc-nav-text">丑小鸭定理 - 特征表示</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#统计学相关概念"><span class="toc-nav-text">统计学相关概念</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#马氏距离"><span class="toc-nav-text">马氏距离</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#高斯分布"><span class="toc-nav-text">高斯分布</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#先验概率"><span class="toc-nav-text">先验概率</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#后验概率"><span class="toc-nav-text">后验概率</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#朴素贝叶斯分类"><span class="toc-nav-text">朴素贝叶斯分类</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#Bias-Variance-两难"><span class="toc-nav-text">Bias - Variance 两难</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#模型选择的次序："><span class="toc-nav-text">模型选择的次序：</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#新型机器学习技术"><span class="toc-nav-text">新型机器学习技术</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#本质（统计机器学习）"><span class="toc-nav-text">本质（统计机器学习）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#核心技术"><span class="toc-nav-text">核心技术</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#2-神经元和感知机"><span class="toc-nav-text">2. 神经元和感知机</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-1-脑和神经元"><span class="toc-nav-text">2.1 脑和神经元</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Hebb法则"><span class="toc-nav-text">Hebb法则</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#神经网络发展历史"><span class="toc-nav-text">神经网络发展历史</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#MP神经元"><span class="toc-nav-text">MP神经元</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#基本模型"><span class="toc-nav-text">基本模型</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#局限性"><span class="toc-nav-text">局限性</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#激励函数"><span class="toc-nav-text">激励函数</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-2-感知器和感知学习"><span class="toc-nav-text">2.2 感知器和感知学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#感知器结构"><span class="toc-nav-text">感知器结构</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#MP神经元组成的集合-输出只有一个"><span class="toc-nav-text">MP神经元组成的集合  输出只有一个</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#非线性前馈网络特点"><span class="toc-nav-text">非线性前馈网络特点</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#修正方法："><span class="toc-nav-text">修正方法：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#输入偏置"><span class="toc-nav-text">输入偏置</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#学习算法"><span class="toc-nav-text">学习算法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#例题：计算更新后的权重"><span class="toc-nav-text">例题：计算更新后的权重</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-3-线性可分性"><span class="toc-nav-text">2.3 线性可分性</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#OR函数"><span class="toc-nav-text">OR函数</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#决策边界"><span class="toc-nav-text">决策边界</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#多分类决策边界-多个输出-多分类"><span class="toc-nav-text">多分类决策边界 - 多个输出 多分类</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#感知机收敛理论"><span class="toc-nav-text">感知机收敛理论</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#感知机缺点"><span class="toc-nav-text">感知机缺点</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#感知机的表达能力"><span class="toc-nav-text">感知机的表达能力</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#感知机缺点-1"><span class="toc-nav-text">感知机缺点</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#感知机方法只适用在单层网络！！"><span class="toc-nav-text">感知机方法只适用在单层网络！！</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#3-神经元网络"><span class="toc-nav-text">3. 神经元网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多层感知机"><span class="toc-nav-text">多层感知机</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#隐层神经元"><span class="toc-nav-text">隐层神经元</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#目的："><span class="toc-nav-text">目的：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#权值的计算方法："><span class="toc-nav-text">权值的计算方法：</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#误差反传（BP，back-propapagation"><span class="toc-nav-text">误差反传（BP，back-propapagation)</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#误差定义："><span class="toc-nav-text">误差定义：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#Delata规则："><span class="toc-nav-text">Delata规则：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#反向传播算法"><span class="toc-nav-text">反向传播算法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多层感知机-→-BP神经网络"><span class="toc-nav-text">多层感知机 → BP神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#【重要】BP误差反差学习传播算法推导"><span class="toc-nav-text">【重要】BP误差反差学习传播算法推导</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#无隐藏层："><span class="toc-nav-text">无隐藏层：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#有隐藏层："><span class="toc-nav-text">有隐藏层：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#sigmoid导数"><span class="toc-nav-text">sigmoid导数</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#其他议题"><span class="toc-nav-text">其他议题</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#自动编码器AUTOEncoder-最早深度学习的起源"><span class="toc-nav-text">自动编码器AUTOEncoder - 最早深度学习的起源</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#径向基网络RBF-有监督学习"><span class="toc-nav-text">径向基网络RBF - 有监督学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#感受野"><span class="toc-nav-text">感受野</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#径向基网络"><span class="toc-nav-text">径向基网络</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#径向基网络的学习"><span class="toc-nav-text">径向基网络的学习</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#径向基网络的原理"><span class="toc-nav-text">径向基网络的原理</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#RBF-vs-BP"><span class="toc-nav-text">RBF vs BP</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#RBF-vs-SVM"><span class="toc-nav-text">RBF vs SVM</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#4-维度约减（降维）"><span class="toc-nav-text">4. 维度约减（降维）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#特征选择和降维"><span class="toc-nav-text">特征选择和降维</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#降维："><span class="toc-nav-text">降维：</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#目的：-1"><span class="toc-nav-text">目的：</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#特征选择（这章不讲）"><span class="toc-nav-text">特征选择（这章不讲）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#LDA-线性判别分析-监督学习"><span class="toc-nav-text">LDA 线性判别分析 - 监督学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法基本流程"><span class="toc-nav-text">算法基本流程</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#PCA-主成分分析-无监督"><span class="toc-nav-text">PCA 主成分分析 - 无监督</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#主成分特点"><span class="toc-nav-text">主成分特点</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#目标函数：最大化样本点在主成分投影上的方差"><span class="toc-nav-text">目标函数：最大化样本点在主成分投影上的方差</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法细节"><span class="toc-nav-text">算法细节</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#拉格朗日乘子法（用来证明要优化的目标）："><span class="toc-nav-text">拉格朗日乘子法（用来证明要优化的目标）：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法过程"><span class="toc-nav-text">算法过程</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#核PCA"><span class="toc-nav-text">核PCA</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#PCA-vs-LDA"><span class="toc-nav-text">PCA vs LDA</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ICA-独立成分分析"><span class="toc-nav-text">ICA 独立成分分析</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#因素分析"><span class="toc-nav-text">因素分析</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#盲源分离（源未知）"><span class="toc-nav-text">盲源分离（源未知）</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法分析"><span class="toc-nav-text">算法分析</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#主要算法"><span class="toc-nav-text">主要算法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#ICA-vs-PCA"><span class="toc-nav-text">ICA vs PCA</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#局部线性嵌入-LLE"><span class="toc-nav-text">局部线性嵌入 LLE</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#核心"><span class="toc-nav-text">核心</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#近邻点的确定"><span class="toc-nav-text">近邻点的确定</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#权重约束"><span class="toc-nav-text">权重约束</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#计算方法"><span class="toc-nav-text">计算方法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ISOMAP-等距特征映射"><span class="toc-nav-text">ISOMAP - 等距特征映射</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#MDS-多维缩放"><span class="toc-nav-text">MDS 多维缩放</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#目标函数"><span class="toc-nav-text">目标函数</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#经典MDS算法"><span class="toc-nav-text">经典MDS算法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#流形空间"><span class="toc-nav-text">流形空间</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#距离"><span class="toc-nav-text">距离</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#测地线距离"><span class="toc-nav-text">测地线距离</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#ISOMAP算法"><span class="toc-nav-text">ISOMAP算法</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#7-优化与搜索"><span class="toc-nav-text">7 优化与搜索</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#数学基础"><span class="toc-nav-text">数学基础</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#梯度下降-寻找导数为0的点"><span class="toc-nav-text">梯度下降 - 寻找导数为0的点</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#原理"><span class="toc-nav-text">原理</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#代表算法"><span class="toc-nav-text">代表算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#冲量法"><span class="toc-nav-text">冲量法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#NAG"><span class="toc-nav-text">NAG</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#自适应梯度法"><span class="toc-nav-text">自适应梯度法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#缺点"><span class="toc-nav-text">缺点</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#优点"><span class="toc-nav-text">优点</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#牛顿法-寻找导数为0的点"><span class="toc-nav-text">牛顿法 - 寻找导数为0的点</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法"><span class="toc-nav-text">算法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#牛顿法改进"><span class="toc-nav-text">牛顿法改进</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#阻尼牛顿法"><span class="toc-nav-text">阻尼牛顿法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#拟牛顿法"><span class="toc-nav-text">拟牛顿法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#牛顿法优点"><span class="toc-nav-text">牛顿法优点</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#缺点-1"><span class="toc-nav-text">缺点</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#最小二乘优化-要求fx形式"><span class="toc-nav-text">最小二乘优化 - 要求fx形式</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#SVD分解法"><span class="toc-nav-text">SVD分解法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#信赖域法"><span class="toc-nav-text">信赖域法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#Levenberg-Marquardt算法"><span class="toc-nav-text">Levenberg-Marquardt算法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法流程"><span class="toc-nav-text">算法流程</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#共轭梯度法"><span class="toc-nav-text">共轭梯度法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#数学定义"><span class="toc-nav-text">数学定义</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法流程-1"><span class="toc-nav-text">算法流程</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#要点"><span class="toc-nav-text">要点</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#搜索"><span class="toc-nav-text">搜索</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#TSP问题-NP完全问题"><span class="toc-nav-text">TSP问题 - NP完全问题</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#可能失效的原因"><span class="toc-nav-text">可能失效的原因</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#利用和探索-搜索的两种机制"><span class="toc-nav-text">利用和探索 - 搜索的两种机制</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#模拟退火算法-启发式算法"><span class="toc-nav-text">模拟退火算法 - 启发式算法</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#UCB算法"><span class="toc-nav-text">UCB算法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#8-演化学习"><span class="toc-nav-text">8 演化学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#遗传算法GA-解决非凸问题的随机优化方式（与前面优化方法的不同）"><span class="toc-nav-text">遗传算法GA - 解决非凸问题的随机优化方式（与前面优化方法的不同）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#4个问题"><span class="toc-nav-text">4个问题</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#一般形式"><span class="toc-nav-text">一般形式</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#假设模型的表示"><span class="toc-nav-text">假设模型的表示</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#应用实例"><span class="toc-nav-text">应用实例</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#产生后代的方法"><span class="toc-nav-text">产生后代的方法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#遗传算子"><span class="toc-nav-text">遗传算子</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#后代演化方法"><span class="toc-nav-text">后代演化方法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#举例"><span class="toc-nav-text">举例</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模式定理"><span class="toc-nav-text">模式定理</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#阶"><span class="toc-nav-text">阶</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#长度"><span class="toc-nav-text">长度</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#模式理论"><span class="toc-nav-text">模式理论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#模式进化"><span class="toc-nav-text">模式进化</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#模式定理-1"><span class="toc-nav-text">模式定理</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#遗传算法求解最短路径问题"><span class="toc-nav-text">遗传算法求解最短路径问题</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#11-强化学习"><span class="toc-nav-text">11 强化学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#本质：奖惩与试错"><span class="toc-nav-text">本质：奖惩与试错</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#学习目标"><span class="toc-nav-text">学习目标</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#MDP模型"><span class="toc-nav-text">MDP模型</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#轨迹"><span class="toc-nav-text">轨迹</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#返回函数"><span class="toc-nav-text">返回函数</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#动作选择"><span class="toc-nav-text">动作选择</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#动态规划"><span class="toc-nav-text">动态规划</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#策略评估"><span class="toc-nav-text">策略评估</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#控制"><span class="toc-nav-text">控制</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Mento-Carlo策略评价"><span class="toc-nav-text">Mento Carlo策略评价</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#时差学习"><span class="toc-nav-text">时差学习</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#监督学习-vs-强化学习"><span class="toc-nav-text">监督学习 vs 强化学习</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#离策略-在策略"><span class="toc-nav-text">离策略 在策略</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#Q-学习算法"><span class="toc-nav-text">Q-学习算法</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#SARSA算法"><span class="toc-nav-text">SARSA算法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#N步TD预测"><span class="toc-nav-text">N步TD预测</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#关系强化学习"><span class="toc-nav-text">关系强化学习</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#10-树学习"><span class="toc-nav-text">10 树学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#符号-概念-学习和变型空间"><span class="toc-nav-text">符号(概念)学习和变型空间</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#概念学习"><span class="toc-nav-text">概念学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#任务"><span class="toc-nav-text">任务</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#实例空间和假设"><span class="toc-nav-text">实例空间和假设</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#搜索的概念学习"><span class="toc-nav-text">搜索的概念学习</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Find-S-（笨方法）-寻找极大特殊假设"><span class="toc-nav-text">Find-S （笨方法）- 寻找极大特殊假设</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#特点"><span class="toc-nav-text">特点</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#变型空间-version"><span class="toc-nav-text">变型空间 - version</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#列表消除算法"><span class="toc-nav-text">列表消除算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#正例-负例的作用"><span class="toc-nav-text">正例 负例的作用</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#候选消除算法-爬山法"><span class="toc-nav-text">候选消除算法 - 爬山法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#Find-S算法只找到了S，而爬山法找到了G，最终结果就在S和G之间"><span class="toc-nav-text">Find-S算法只找到了S，而爬山法找到了G，最终结果就在S和G之间</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#归纳偏置-Inductive-bias-给定某种形式的预先假定"><span class="toc-nav-text">归纳偏置 Inductive bias - 给定某种形式的预先假定</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#无偏"><span class="toc-nav-text">无偏</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#三种归纳学习算法"><span class="toc-nav-text">三种归纳学习算法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#决策树算法"><span class="toc-nav-text">决策树算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#决策树"><span class="toc-nav-text">决策树</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#ID3算法"><span class="toc-nav-text">ID3算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#信息增益"><span class="toc-nav-text">信息增益</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#特点-1"><span class="toc-nav-text">特点</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#决策树中的归纳偏置"><span class="toc-nav-text">决策树中的归纳偏置</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#其他树算法"><span class="toc-nav-text">其他树算法</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#11-集成学习"><span class="toc-nav-text">11 集成学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#原理-1"><span class="toc-nav-text">原理</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#特点-2"><span class="toc-nav-text">特点</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-6"><a class="toc-nav-link" href="#准确度和多样性之间要有权衡"><span class="toc-nav-text">准确度和多样性之间要有权衡</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#核心问题"><span class="toc-nav-text">核心问题</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#结合策略"><span class="toc-nav-text">结合策略</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#多样性策略"><span class="toc-nav-text">多样性策略</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Bagging-（Bootstrap-Aggregation）"><span class="toc-nav-text">Bagging （Bootstrap Aggregation）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#代表算法-随机森林"><span class="toc-nav-text">代表算法 - 随机森林</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#构建RF方法："><span class="toc-nav-text">构建RF方法：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#优点-1"><span class="toc-nav-text">优点</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Boosting"><span class="toc-nav-text">Boosting</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#PAC学习理论"><span class="toc-nav-text">PAC学习理论</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#GBDT"><span class="toc-nav-text">GBDT</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法流程-2"><span class="toc-nav-text">算法流程</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Adaboost"><span class="toc-nav-text">Adaboost</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#PAC学习理论-1"><span class="toc-nav-text">PAC学习理论</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Stacking"><span class="toc-nav-text">Stacking</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#12-无监督学习"><span class="toc-nav-text">12 无监督学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#聚类相关概念"><span class="toc-nav-text">聚类相关概念</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#影响聚类的因素："><span class="toc-nav-text">影响聚类的因素：</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#距离度量"><span class="toc-nav-text">距离度量</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#常用度量函数"><span class="toc-nav-text">常用度量函数</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#聚类准则"><span class="toc-nav-text">聚类准则</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#聚类方法"><span class="toc-nav-text">聚类方法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#影响因素"><span class="toc-nav-text">影响因素</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法过程-1"><span class="toc-nav-text">算法过程</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#过程："><span class="toc-nav-text">过程：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法流程-3"><span class="toc-nav-text">算法流程</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#聚类评价"><span class="toc-nav-text">聚类评价</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#常用评价指标"><span class="toc-nav-text">常用评价指标</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#标签未知"><span class="toc-nav-text">标签未知</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#标签已知（聚类结果和真实标签的差异）"><span class="toc-nav-text">标签已知（聚类结果和真实标签的差异）</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#可视化"><span class="toc-nav-text">可视化</span></a></li></ol></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#概率与学习"><span class="toc-nav-text">概率与学习</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#相关概念"><span class="toc-nav-text">相关概念</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#随机变量"><span class="toc-nav-text">随机变量</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#琴生不等式"><span class="toc-nav-text">琴生不等式</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#高斯分布-正态分布"><span class="toc-nav-text">高斯分布/正态分布</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#高斯混合模型-Gaussian-Mixture-Model"><span class="toc-nav-text">高斯混合模型 Gaussian Mixture Model</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#最大似然估计-Maximum-likelihood-estimation-MLE"><span class="toc-nav-text">最大似然估计 Maximum likelihood estimation MLE</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法过程-2"><span class="toc-nav-text">算法过程</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#期望最大化算法-Expectiation-Maximization"><span class="toc-nav-text">期望最大化算法 Expectiation-Maximization</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#思想"><span class="toc-nav-text">思想</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#E-step"><span class="toc-nav-text">E-step</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#M-step"><span class="toc-nav-text">M-step</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Repeat"><span class="toc-nav-text">Repeat</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#优化分析"><span class="toc-nav-text">优化分析</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#EM-vs-K-means"><span class="toc-nav-text">EM vs K-means</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#K-means背后的EM思想"><span class="toc-nav-text">K-means背后的EM思想</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#概率与学习（续）"><span class="toc-nav-text">概率与学习（续）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#回顾"><span class="toc-nav-text">回顾</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#KNN"><span class="toc-nav-text">KNN</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#算法流程："><span class="toc-nav-text">算法流程：</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#k的取值的影响"><span class="toc-nav-text">k的取值的影响</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#最近邻分类器-1-NN"><span class="toc-nav-text">最近邻分类器 1-NN</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#泛化错误率"><span class="toc-nav-text">泛化错误率</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#K-近邻回归"><span class="toc-nav-text">K-近邻回归</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#近邻平滑"><span class="toc-nav-text">近邻平滑</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#讨论"><span class="toc-nav-text">讨论</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#懒惰学习"><span class="toc-nav-text">懒惰学习</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#优点-2"><span class="toc-nav-text">优点</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#缺点-2"><span class="toc-nav-text">缺点</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#时间复杂度"><span class="toc-nav-text">时间复杂度</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#降低近邻计算"><span class="toc-nav-text">降低近邻计算</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#构造流程"><span class="toc-nav-text">构造流程</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#KD-Tree-搜索"><span class="toc-nav-text">KD-Tree 搜索</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#时间复杂度-1"><span class="toc-nav-text">时间复杂度</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#扩展阅读"><span class="toc-nav-text">扩展阅读</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#SVM"><span class="toc-nav-text">SVM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#回顾-1"><span class="toc-nav-text">回顾</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#概率框架角度"><span class="toc-nav-text">概率框架角度</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#感知机"><span class="toc-nav-text">感知机</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#线性SVM"><span class="toc-nav-text">线性SVM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#间隔与支持向量"><span class="toc-nav-text">间隔与支持向量</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#推导。。。"><span class="toc-nav-text">推导。。。</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#soft-margin"><span class="toc-nav-text">soft margin</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#惩罚机制："><span class="toc-nav-text">惩罚机制：</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#非线性SVM"><span class="toc-nav-text">非线性SVM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#kernal-trick"><span class="toc-nav-text">kernal trick</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#Mercer’s-condition-充分必要"><span class="toc-nav-text">Mercer’s condition 充分必要</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Kernel-SVM"><span class="toc-nav-text">Kernel SVM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#非线性核"><span class="toc-nav-text">非线性核</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#超参数"><span class="toc-nav-text">超参数</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#多类SVM"><span class="toc-nav-text">多类SVM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#多类"><span class="toc-nav-text">多类</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#1对多"><span class="toc-nav-text">1对多</span></a></li></ol></li></ol></li></ol></li></ol>
    
  </div>
</aside>
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'http://yoursite.com/2020/10/11/机器学习/';
    var banner = ''
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

    // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()

        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })
  })();
</script>







    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    &copy; 2021 Content by <a href="https://github.com/Tianchenjiang" target="_blank">Mars Tian</a> | Proudly powered by <a href="https://hexo.io" target="_blank">Hexo</a></p>
</footer>

<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>
  </body>
</html>